"""
Script cháº¡y toÃ n bá»™ pipeline: Generate data -> Train -> Test
Há»‡ thá»‘ng nháº­n dáº¡ng má»i cÆ¡ vá»›i LDA, KNN, SVM
"""

import os
import sys
import argparse
from datetime import datetime

# Import cÃ¡c modules
from generate_data import save_train_test_data
from train_models import train_all_models
from test_models import test_all_models

def run_full_pipeline(n_samples=2000, test_size=0.25, use_grid_search=True, seed=42):
    """
    Cháº¡y toÃ n bá»™ pipeline tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i

    Parameters:
    -----------
    n_samples : int
        Sá»‘ lÆ°á»£ng máº«u táº¡o ra
    test_size : float
        Tá»· lá»‡ test set
    use_grid_search : bool
        Sá»­ dá»¥ng GridSearchCV
    seed : int
        Random seed
    """
    print("="*70)
    print(" "*10 + "Há»† THá»NG NHáº¬N Dáº NG Má»I CÆ  - FULL PIPELINE")
    print(" "*15 + "LDA | KNN | SVM")
    print("="*70)

    start_time = datetime.now()

    # Táº¡o thÆ° má»¥c output
    data_dir = 'data_generated'
    models_dir = 'models'
    plots_dir = 'plots'
    test_results_dir = 'test_results'

    for dir_path in [data_dir, models_dir, plots_dir, test_results_dir]:
        os.makedirs(dir_path, exist_ok=True)

    # ============= BÆ¯á»šC 1: GENERATE DATA =============
    print(f"\n{'='*70}")
    print("BÆ¯á»šC 1: GENERATE SYNTHETIC DATA")
    print('='*70)

    train_df, test_df, full_df = save_train_test_data(
        output_dir=data_dir,
        n_samples=n_samples,
        test_size=test_size,
        seed=seed
    )

    train_data_path = os.path.join(data_dir, 'train_data.csv')
    test_data_path = os.path.join(data_dir, 'test_data.csv')

    # ============= BÆ¯á»šC 2: TRAIN MODELS =============
    print(f"\n{'='*70}")
    print("BÆ¯á»šC 2: TRAIN MODELS (LDA, KNN, SVM)")
    print('='*70)

    train_results, train_comparison = train_all_models(
        train_data_path=train_data_path,
        test_data_path=test_data_path,
        use_grid_search=use_grid_search,
        output_dir=models_dir,
        plot_dir=plots_dir
    )

    # ============= BÆ¯á»šC 3: TEST MODELS =============
    print(f"\n{'='*70}")
    print("BÆ¯á»šC 3: TEST MODELS")
    print('='*70)

    test_results, test_comparison = test_all_models(
        test_data_path=test_data_path,
        models_dir=models_dir,
        output_dir=test_results_dir
    )

    # ============= Tá»”NG Káº¾T =============
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"\n{'='*70}")
    print("Tá»”NG Káº¾T Káº¾T QUáº¢")
    print('='*70)

    print(f"\nğŸ“Š Sá»‘ liá»‡u dá»¯ liá»‡u:")
    print(f"  - Tá»•ng sá»‘ máº«u: {n_samples}")
    print(f"  - Train set: {len(train_df)} máº«u")
    print(f"  - Test set: {len(test_df)} máº«u")

    print(f"\nğŸ¯ Káº¿t quáº£ Test (cÃ¡c models):")
    print(test_comparison)

    # TÃ¬m model tá»‘t nháº¥t
    best_model = test_comparison['Accuracy'].idxmax()
    best_accuracy = test_comparison.loc[best_model, 'Accuracy']

    print(f"\nğŸ† Model tá»‘t nháº¥t: {best_model.upper()}")
    print(f"   - Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
    print(f"   - Precision: {test_comparison.loc[best_model, 'Precision']:.4f}")
    print(f"   - Recall: {test_comparison.loc[best_model, 'Recall']:.4f}")
    print(f"   - F1-Score: {test_comparison.loc[best_model, 'F1-Score']:.4f}")

    # Kiá»ƒm tra Ä‘áº¡t má»¥c tiÃªu
    print(f"\nğŸ“ˆ ÄÃ¡nh giÃ¡ má»¥c tiÃªu (Accuracy >= 85%):")
    for model_name in test_comparison.index:
        acc = test_comparison.loc[model_name, 'Accuracy']
        status = "âœ“ Äáº T" if acc >= 0.85 else "âœ— CHÆ¯A Äáº T"
        print(f"   - {model_name.upper()}: {acc:.4f} ({acc*100:.2f}%) - {status}")

    # Kiá»ƒm tra xem cÃ³ model nÃ o Ä‘áº¡t target khÃ´ng
    models_è¾¾æ ‡ = test_comparison[test_comparison['Accuracy'] >= 0.85]
    if len(models_è¾¾æ ‡) > 0:
        print(f"\nâœ“âœ“âœ“ THÃ€NH CÃ”NG! CÃ³ {len(models_è¾¾æ ‡)}/{len(test_comparison)} model(s) Ä‘áº¡t má»¥c tiÃªu >= 85% âœ“âœ“âœ“")
    else:
        print(f"\nâš  ChÆ°a cÃ³ model nÃ o Ä‘áº¡t má»¥c tiÃªu 85%. Khuyáº¿n nghá»‹:")
        print("  1. TÄƒng sá»‘ lÆ°á»£ng máº«u training (n_samples)")
        print("  2. ThÃªm features quan trá»ng hÆ¡n")
        print("  3. Äiá»u chá»‰nh param grid cho GridSearchCV")
        print("  4. Thá»­ feature engineering")

    print(f"\nâ± Thá»i gian cháº¡y: {duration:.2f} giÃ¢y ({duration/60:.2f} phÃºt)")

    print(f"\nğŸ“ CÃ¡c file output:")
    print(f"  - Data: {data_dir}/")
    print(f"  - Models: {models_dir}/")
    print(f"  - Plots: {plots_dir}/")
    print(f"  - Test results: {test_results_dir}/")

    print(f"\n{'='*70}")
    print("âœ“ HOÃ€N Táº¤T TOÃ€N Bá»˜ PIPELINE!")
    print('='*70)

    return {
        'train_results': train_results,
        'test_results': test_results,
        'train_comparison': train_comparison,
        'test_comparison': test_comparison,
        'best_model': best_model,
        'best_accuracy': best_accuracy,
        'duration': duration
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Cháº¡y toÃ n bá»™ pipeline nháº­n dáº¡ng má»i cÆ¡')
    parser.add_argument('--n-samples', type=int, default=2000,
                       help='Sá»‘ lÆ°á»£ng máº«u táº¡o ra (máº·c Ä‘á»‹nh: 2000)')
    parser.add_argument('--test-size', type=float, default=0.25,
                       help='Tá»· lá»‡ test set (máº·c Ä‘á»‹nh: 0.25)')
    parser.add_argument('--no-grid-search', action='store_true',
                       help='KhÃ´ng sá»­ dá»¥ng GridSearchCV (train nhanh hÆ¡n)')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed (máº·c Ä‘á»‹nh: 42)')

    args = parser.parse_args()

    # Cháº¡y pipeline
    results = run_full_pipeline(
        n_samples=args.n_samples,
        test_size=args.test_size,
        use_grid_search=not args.no_grid_search,
        seed=args.seed
    )
