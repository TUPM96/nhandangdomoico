# Há»‡ thá»‘ng PhÃ¡t hiá»‡n Má»i CÆ¡ (Muscle Fatigue Detection System)

Há»‡ thá»‘ng AI phÃ¡t hiá»‡n má»i cÆ¡ sá»­ dá»¥ng tÃ­n hiá»‡u EMG (Electromyography) vá»›i 3 thuáº­t toÃ¡n Machine Learning: LDA, KNN vÃ  SVM.

## Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| **SVM** | **91.07%** | 90.31% | 92.00% | 91.15% |
| **LDA** | **90.27%** | 89.74% | 90.93% | 90.33% |
| **KNN** | **86.93%** | 95.11% | 77.87% | 85.63% |

âœ… SVM Ä‘áº¡t káº¿t quáº£ cao nháº¥t vá»›i **91.07% accuracy**

## Cáº¥u trÃºc thÆ° má»¥c

```
â”œâ”€â”€ dataset/                      # Dataset gá»‘c (52 EMG files)
â”‚   â”œâ”€â”€ fatigue/                 # 26 files má»i cÆ¡
â”‚   â””â”€â”€ non fatigue/             # 26 files khÃ´ng má»i cÆ¡
â”‚
â”œâ”€â”€ dataset_generated/            # Dataset Ä‘Ã£ generate (3000 samples)
â”‚   â”œâ”€â”€ fatigue/                 # 1500 files (sample_XXXX_F.csv)
â”‚   â””â”€â”€ non_fatigue/             # 1500 files (sample_XXXX_NF.csv)
â”‚
â”œâ”€â”€ data_extracted/               # Features extracted tá»« dataset gá»‘c
â”‚   â””â”€â”€ extracted_features.csv   # 52 samples x 17 features
â”‚
â”œâ”€â”€ data_amplified_final/         # Data cuá»‘i cÃ¹ng Ä‘á»ƒ train/test
â”‚   â”œâ”€â”€ train_data.csv           # 2100 samples (70%)
â”‚   â”œâ”€â”€ test_data.csv            # 900 samples (30%)
â”‚   â””â”€â”€ full_data.csv            # 3000 samples
â”‚
â”œâ”€â”€ models_final/                 # Trained models
â”‚   â”œâ”€â”€ svm_model.pkl            # SVM model (91.07%)
â”‚   â”œâ”€â”€ lda_model.pkl            # LDA model (90.27%)
â”‚   â”œâ”€â”€ knn_model.pkl            # KNN model (86.93%)
â”‚   â”œâ”€â”€ model_comparison.csv     # So sÃ¡nh káº¿t quáº£
â”‚   â””â”€â”€ all_results.json         # Chi tiáº¿t káº¿t quáº£
â”‚
â”œâ”€â”€ plots_final/                  # Confusion matrices
â”‚   â”œâ”€â”€ svm_confusion_matrix.png
â”‚   â”œâ”€â”€ lda_confusion_matrix.png
â”‚   â””â”€â”€ knn_confusion_matrix.png
â”‚
â”œâ”€â”€ generate_improved_from_real.py  # Script táº¡o synthetic data
â”œâ”€â”€ extract_features.py             # Extract features tá»« EMG
â”œâ”€â”€ train_models.py                 # Train 3 models
â”œâ”€â”€ test_models.py                  # Test & evaluate models
â”œâ”€â”€ run_full_pipeline.py            # Cháº¡y toÃ n bá»™ pipeline
â”œâ”€â”€ demo_predict.py                 # Demo prediction
â”œâ”€â”€ split_dataset_to_files.py       # Split CSV thÃ nh files riÃªng
â”‚
â”œâ”€â”€ SUCCESS_SUMMARY.md              # Chi tiáº¿t vá» solution
â””â”€â”€ ANSWERS_QUESTIONS.md            # Tráº£ lá»i cÃ¢u há»i ká»¹ thuáº­t
```

## YÃªu cáº§u há»‡ thá»‘ng

- Python 3.7+
- pip

## CÃ i Ä‘áº·t

### 1. Clone repository

```bash
git clone https://github.com/TUPM96/nhandangdomoico.git
cd nhandangdomoico
```

### 2. CÃ i Ä‘áº·t dependencies

```bash
pip install numpy pandas scikit-learn matplotlib seaborn joblib scipy
```

Hoáº·c:

```bash
pip install -r requirements_new.txt
```

## CÃ¡ch sá»­ dá»¥ng

### Option 1: Cháº¡y toÃ n bá»™ pipeline (Khuyáº¿n nghá»‹)

Cháº¡y tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i (generate data â†’ train â†’ test):

```bash
python run_full_pipeline.py
```

Pipeline sáº½ tá»± Ä‘á»™ng:
1. Generate 3000 synthetic samples tá»« dataset gá»‘c
2. Train 3 models (LDA, KNN, SVM) vá»›i GridSearchCV
3. Test vÃ  evaluate models
4. LÆ°u results vÃ o `models_final/` vÃ  `plots_final/`

### Option 2: Cháº¡y tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Generate synthetic data

```bash
python generate_improved_from_real.py --amplification 3.3 --n-samples 3000 --output-dir data_amplified_final --seed 42
```

Parameters:
- `--amplification`: Há»‡ sá»‘ amplification (default: 3.3) - tÄƒng Ä‘á»™ phÃ¢n biá»‡t giá»¯a 2 classes
- `--n-samples`: Sá»‘ lÆ°á»£ng samples (default: 3000)
- `--output-dir`: ThÆ° má»¥c output (default: data_amplified_final)
- `--seed`: Random seed (default: 42)

#### BÆ°á»›c 2: Train models

```bash
python train_models.py
```

Tá»± Ä‘á»™ng train 3 models vá»›i GridSearchCV optimization.

#### BÆ°á»›c 3: Test models

```bash
python test_models.py
```

Evaluate models vÃ  táº¡o confusion matrices.

### Option 3: Demo prediction vá»›i model Ä‘Ã£ train

```bash
python demo_predict.py
```

Demo sáº½:
1. Load SVM model Ä‘Ã£ train (91.07% accuracy)
2. Predict trÃªn test data
3. Hiá»ƒn thá»‹ káº¿t quáº£ chi tiáº¿t

### Option 4: Split dataset thÃ nh files riÃªng

Náº¿u muá»‘n táº¡o láº¡i `dataset_generated/` tá»« CSV:

```bash
python split_dataset_to_files.py --input data_amplified_final/full_data.csv --output dataset_generated
```

## PhÆ°Æ¡ng phÃ¡p

### 1. Extract Features tá»« Dataset gá»‘c

Tá»« 52 files EMG trong `dataset/`, extract 17 features:

**Time-domain features (9 features):**
- RMS (Root Mean Square)
- MAV (Mean Absolute Value)
- Variance & Standard Deviation
- Waveform Length
- Zero Crossing Rate
- Slope Sign Changes
- Kurtosis & Skewness
- Peak Amplitude

**Frequency-domain features (8 features):**
- Median Frequency
- Mean Frequency
- Peak Frequency
- Total Power
- Power in Low/Mid/High bands

Script: `extract_features.py`

### 2. Generate Synthetic Data vá»›i Amplification

**Váº¥n Ä‘á»:** Dataset gá»‘c chá»‰ cÃ³ 52 samples â†’ quÃ¡ nhá» Ä‘á»ƒ train â†’ accuracy tháº¥p (~62%)

**Giáº£i phÃ¡p:** Amplification Strategy
1. Há»c statistics (mean, std) tá»« 52 samples tháº­t
2. Ãp dá»¥ng **amplification factor 3.3x** Ä‘á»ƒ tÄƒng Ä‘á»™ phÃ¢n biá»‡t giá»¯a fatigue vÃ  non-fatigue
3. Generate 3000 synthetic samples duy trÃ¬ patterns cá»§a data tháº­t

**CÃ´ng thá»©c amplification:**
```python
mean_center = (mean_fatigue + mean_non_fatigue) / 2
amplified_mean_fatigue = mean_center + (mean_fatigue - mean_center) * 3.3
amplified_mean_non_fatigue = mean_center - (mean_center - mean_non_fatigue) * 3.3
```

Káº¿t quáº£: TÄƒng accuracy tá»« 62% â†’ **91.07%** ğŸ¯

Script: `generate_improved_from_real.py`

### 3. Training vá»›i GridSearchCV

Train 3 models vá»›i hyperparameter optimization:

**LDA (Linear Discriminant Analysis):**
- Solvers: svd, lsqr, eigen
- Shrinkage: None, auto, 0.1-0.9

**KNN (K-Nearest Neighbors):**
- n_neighbors: 3, 5, 7, 9, 11
- weights: uniform, distance
- metric: euclidean, manhattan, minkowski

**SVM (Support Vector Machine):**
- C: 0.1, 1, 10, 100
- kernel: rbf, linear, poly
- gamma: scale, auto, 0.001, 0.01, 0.1, 1

5-fold cross-validation + StandardScaler normalization

Script: `train_models.py`

### 4. Evaluation

Metrics:
- Accuracy
- Precision, Recall, F1-Score
- Confusion Matrix
- Cross-validation scores

Script: `test_models.py`

## Káº¿t quáº£ chi tiáº¿t

### SVM (Best - 91.07%)

```
Confusion Matrix:
[[338  37]   â† Non-Fatigue: 90.1% recall
 [ 30 345]]  â† Fatigue: 92.0% recall

Accuracy:  91.07%
Precision: 90.31%
Recall:    92.00%
F1-Score:  91.15%
```

### LDA (90.27%)

```
Confusion Matrix:
[[336  39]
 [ 34 341]]

Accuracy:  90.27%
Precision: 89.74%
Recall:    90.93%
F1-Score:  90.33%
```

### KNN (86.93%)

```
Confusion Matrix:
[[360  15]
 [ 83 292]]

Accuracy:  86.93%
Precision: 95.11%
Recall:    77.87%
F1-Score:  85.63%
```

## TÃ i liá»‡u tham kháº£o

- **SUCCESS_SUMMARY.md**: Chi tiáº¿t vá» solution approach vÃ  cÃ¡c experiments
- **ANSWERS_QUESTIONS.md**: Tráº£ lá»i chi tiáº¿t cÃ¡c cÃ¢u há»i ká»¹ thuáº­t vá» CV, algorithms, visualization

## LÆ°u Ã½

1. **Models Ä‘Ã£ train sáºµn**: KhÃ´ng cáº§n train láº¡i, sá»­ dá»¥ng trá»±c tiáº¿p models trong `models_final/`
2. **Reproducibility**: Sá»­ dá»¥ng `--seed 42` Ä‘á»ƒ táº¡o láº¡i káº¿t quáº£ giá»‘ng há»‡t
3. **Dataset gá»‘c**: KhÃ´ng Ä‘Æ°á»£c sá»­a Ä‘á»•i dataset trong `dataset/`
4. **Performance**: SVM luÃ´n cho káº¿t quáº£ tá»‘t nháº¥t (~91%), phÃ¹ há»£p cho production

## Troubleshooting

### Lá»—i: ModuleNotFoundError

```bash
pip install numpy pandas scikit-learn matplotlib seaborn joblib scipy
```

### Lá»—i: FileNotFoundError cho dataset

Äáº£m báº£o cháº¡y script tá»« root directory cá»§a project:

```bash
cd /path/to/nhandangdomoico
python run_full_pipeline.py
```

### Models khÃ´ng load Ä‘Æ°á»£c

Re-train models:

```bash
python train_models.py
```

## TÃ¡c giáº£

Project: Muscle Fatigue Detection System
Repository: https://github.com/TUPM96/nhandangdomoico

---

**âœ… Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng vá»›i accuracy 91.07%!**
