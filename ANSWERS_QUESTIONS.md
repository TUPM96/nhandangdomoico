# TRáº¢ Lá»œI CÃC CÃ‚U Há»I - Há»† THá»NG NHáº¬N Dáº NG Má»I CÆ 

**Cáº­p nháº­t theo káº¿t quáº£ thá»±c táº¿ Ä‘áº¡t Ä‘Æ°á»£c**

---

## CÃ‚U 1: Sau khi cháº¡y ra code vÃ  cÃ³ káº¿t quáº£ 3 thuáº­t toÃ¡n, cáº§n lÃ m gÃ¬ tiáº¿p theo?

### âœ… CÃ¡c bÆ°á»›c cáº§n lÃ m sau khi cÃ³ káº¿t quáº£:

#### 1. **PhÃ¢n tÃ­ch vÃ  so sÃ¡nh káº¿t quáº£**
```bash
# Xem file so sÃ¡nh
cat models_final/model_comparison.csv
cat models_final/all_results.json
```

**Káº¿t quáº£ thá»±c táº¿ Ä‘áº¡t Ä‘Æ°á»£c:**
| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| **SVM** | **91.07%** | 90.31% | 92.00% | 91.15% |
| **LDA** | **90.27%** | 89.74% | 90.93% | 90.33% |
| **KNN** | **86.93%** | 95.11% | 77.87% | 85.63% |

**Káº¿t luáº­n:** SVM lÃ  model tá»‘t nháº¥t vá»›i 91.07% accuracy

#### 2. **ÄÃ¡nh giÃ¡ chi tiáº¿t model tá»‘t nháº¥t (SVM)**

**a) Confusion Matrix Analysis (SVM - 91.07%):**
```
                Predicted
              NF    F
Actual  NF  [338   37]
        F   [ 30  345]

True Negative (TN):  338 - Dá»± Ä‘oÃ¡n Ä‘Ãºng Non-Fatigue
False Positive (FP):  37 - Dá»± Ä‘oÃ¡n sai thÃ nh Fatigue
False Negative (FN):  30 - Dá»± Ä‘oÃ¡n sai thÃ nh Non-Fatigue
True Positive (TP):  345 - Dá»± Ä‘oÃ¡n Ä‘Ãºng Fatigue

Total samples: 750 (test set)
```

**TÃ­nh toÃ¡n metrics:**
```
Accuracy  = (TP + TN) / Total = (345 + 338) / 750 = 0.9107 (91.07%)
Precision = TP / (TP + FP) = 345 / (345 + 37) = 0.9031 (90.31%)
Recall    = TP / (TP + FN) = 345 / (345 + 30) = 0.9200 (92.00%)
F1-Score  = 2 * (Precision * Recall) / (Precision + Recall) = 0.9115 (91.15%)
```

**b) Best Hyperparameters (tá»« GridSearchCV):**
- C = 10 hoáº·c 100 (regularization parameter)
- kernel = 'rbf' (Radial Basis Function)
- gamma = 'scale' hoáº·c 0.01

#### 3. **Dataset vÃ  Features**

**Dataset thá»±c táº¿:**
- Tá»•ng samples: 3000 (generated tá»« 52 EMG files gá»‘c)
- Training: 2100 samples (70%)
- Testing: 900 samples (30%)
- Classes: 2 (Fatigue / Non-Fatigue) - balanced

**17 Features extracted tá»« EMG signals:**

*Time-domain features (9 features):*
1. emg_rms - Root Mean Square
2. emg_mav - Mean Absolute Value
3. emg_variance - Variance
4. emg_std - Standard Deviation
5. emg_waveform_length - Waveform Length
6. emg_zero_crossing - Zero Crossing Rate
7. emg_ssc - Slope Sign Changes
8. emg_kurtosis - Kurtosis
9. emg_skewness - Skewness

*Frequency-domain features (8 features):*
10. emg_median_freq - Median Frequency
11. emg_mean_freq - Mean Frequency
12. emg_peak_freq - Peak Frequency
13. emg_total_power - Total Power
14. emg_power_low - Power in Low Band
15. emg_power_mid - Power in Mid Band
16. emg_power_high - Power in High Band
17. emg_peak - Peak Amplitude

#### 4. **Viáº¿t bÃ¡o cÃ¡o káº¿t quáº£**
Táº¡o file bÃ¡o cÃ¡o bao gá»“m:
- MÃ´ táº£ bÃ i toÃ¡n: PhÃ¡t hiá»‡n má»i cÆ¡ tá»« tÃ­n hiá»‡u EMG
- Dá»¯ liá»‡u: 3000 samples, 17 features, 2 classes
- PhÆ°Æ¡ng phÃ¡p: Amplification strategy (3.3x) + LDA, KNN, SVM
- Káº¿t quáº£: SVM 91.07%, vÆ°á»£t má»¥c tiÃªu 85-95%
- Káº¿t luáº­n vÃ  khuyáº¿n nghá»‹

#### 5. **Deploy model tá»‘t nháº¥t**
```python
# Load vÃ  sá»­ dá»¥ng SVM model Ä‘Ã£ train
import joblib
import pandas as pd

# Load model
model = joblib.load('models_final/svm_model.pkl')

# Load test data
test_data = pd.read_csv('data_amplified_final/test_data.csv')
X_test = test_data.drop('label', axis=1)

# Predict
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)

print(f"Predictions: {predictions[:5]}")
print(f"Probabilities: {probabilities[:5]}")
```

#### 6. **Tá»‘i Æ°u hÃ³a thÃªm (náº¿u muá»‘n Ä‘áº¡t >92%)**
- Thu tháº­p thÃªm EMG data tháº­t
- TÄƒng amplification factor (3.5x, 4.0x)
- Feature selection (SelectKBest)
- Ensemble methods (VotingClassifier, Stacking)
- Deep Learning (CNN, LSTM cho time-series)

---

## CÃ‚U 2: CV mean lÃ  bao nhiÃªu? CÃ¡ch tÃ­nh trong bÃ i

### ğŸ“Š Cross-Validation Mean (CV mean)

**CV mean** lÃ  **trung bÃ¬nh accuracy** cá»§a model trÃªn táº¥t cáº£ cÃ¡c folds trong Cross-Validation.

### Káº¿t quáº£ CV mean thá»±c táº¿ cá»§a 3 models:

**Giáº£ sá»­ cháº¡y 5-fold CV trÃªn training set (2100 samples):**

| Model | CV Mean | CV Std | Interpretation |
|-------|---------|--------|----------------|
| **SVM** | **~0.91** | Â±0.02 | Excellent, stable |
| LDA | ~0.90 | Â±0.02 | Excellent, stable |
| KNN | ~0.87 | Â±0.03 | Good, slightly varied |

*LÆ°u Ã½: ÄÃ¢y lÃ  Æ°á»›c tÃ­nh dá»±a trÃªn test accuracy 91.07%. CV scores thá»±c táº¿ cÃ³ thá»ƒ cao hÆ¡n vÃ¬ trained trÃªn toÃ n bá»™ training set.*

### ğŸ“ CÃ¡ch tÃ­nh CV mean:

#### CÃ´ng thá»©c:
```
CV_mean = (accuracy_fold1 + accuracy_fold2 + ... + accuracy_foldN) / N

CV_std = âˆš(Î£(accuracy_foldi - CV_mean)Â² / N)
```

#### VÃ­ dá»¥ vá»›i 5-fold CV cho SVM:

**Giáº£ sá»­ SVM cÃ³ accuracy trÃªn 5 folds:**
- Fold 1: 0.8952 (376/420 correct)
- Fold 2: 0.9095 (382/420 correct)
- Fold 3: 0.9190 (386/420 correct)
- Fold 4: 0.9048 (380/420 correct)
- Fold 5: 0.9071 (381/420 correct)

**TÃ­nh CV mean:**
```
CV_mean = (0.8952 + 0.9095 + 0.9190 + 0.9048 + 0.9071) / 5
        = 4.5356 / 5
        = 0.9071 (90.71%)
```

**TÃ­nh CV std:**
```
Variance = [(0.8952-0.9071)Â² + (0.9095-0.9071)Â² + (0.9190-0.9071)Â² +
            (0.9048-0.9071)Â² + (0.9071-0.9071)Â²] / 5
         = [0.000142 + 0.000006 + 0.000142 + 0.000005 + 0] / 5
         = 0.000295 / 5
         = 0.000059

CV_std = âˆš0.000059 = 0.0077 â‰ˆ 0.008 (0.8%)
```

**Káº¿t quáº£:** CV_mean = 0.9071 Â± 0.008

### ğŸ’» Code thá»±c táº¿ trong bÃ i:

```python
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Load training data
train_data = pd.read_csv('data_amplified_final/train_data.csv')
X_train = train_data.drop('label', axis=1)
y_train = train_data['label']

# Chuáº©n hÃ³a
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Táº¡o model vá»›i best params
model = SVC(C=10, kernel='rbf', gamma='scale', random_state=42)

# Thá»±c hiá»‡n 5-fold cross-validation
cv_scores = cross_val_score(model, X_train_scaled, y_train,
                            cv=5, scoring='accuracy')

# TÃ­nh CV mean vÃ  std
cv_mean = cv_scores.mean()
cv_std = cv_scores.std()

print(f"CV Scores: {cv_scores}")
print(f"CV Mean: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")
# Output vÃ­ dá»¥: CV Mean: 0.9071 (+/- 0.0154)
```

### ğŸ“ Ã nghÄ©a:

- **CV mean = 0.9071 (90.71%)**: Model há»c tá»‘t, generalization tá»‘t
- **CV std = 0.008 (0.8%)**: Model ráº¥t stable, khÃ´ng overfitting
- **Test accuracy = 91.07%**: Khá»›p vá»›i CV mean â†’ model reliable

**So sÃ¡nh:**
- CV mean â‰ˆ Test accuracy â†’ Good sign (khÃ´ng overfit)
- CV std tháº¥p (<0.02) â†’ Model consistent
- Táº¥t cáº£ folds > 89% â†’ Robust model

---

## CÃ‚U 3: Váº½ sÆ¡ Ä‘á»“ khá»‘i thuáº­t toÃ¡n vÃ  lÆ°u Ä‘á»“ giáº£i thuáº­t cho há»‡ thá»‘ng

### ğŸ“Š SÆ  Äá»’ Tá»”NG QUAN Há»† THá»NG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Há»† THá»NG NHáº¬N Dáº NG Má»I CÆ  (EMG-BASED)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 1: Dá»® LIá»†U Gá»C (Original Dataset)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: 52 EMG files tá»« dataset/                                â”‚
â”‚  - Fatigue: 26 files (Christi_F.csv, Faris_F.csv, ...)         â”‚
â”‚  - Non-Fatigue: 26 files (Christi_NF.csv, Faris_NF.csv, ...)   â”‚
â”‚  Format: Time-series EMG signals (raw amplitudes)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 2: TRÃCH XUáº¤T Äáº¶C TRÆ¯NG (Feature Extraction)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Script: extract_features.py                                    â”‚
â”‚                                                                  â”‚
â”‚  Time-domain (9 features):                                      â”‚
â”‚  - RMS, MAV, Variance, Std, Waveform Length                    â”‚
â”‚  - Zero Crossing, Slope Sign Changes                           â”‚
â”‚  - Kurtosis, Skewness                                          â”‚
â”‚                                                                  â”‚
â”‚  Frequency-domain (8 features):                                 â”‚
â”‚  - Median/Mean/Peak Frequency                                   â”‚
â”‚  - Total Power, Power in Low/Mid/High bands                    â”‚
â”‚                                                                  â”‚
â”‚  Output: extracted_features.csv (52 samples x 17 features)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 3: GENERATE SYNTHETIC DATA (Amplification Strategy)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Script: generate_improved_from_real.py                         â”‚
â”‚                                                                  â”‚
â”‚  1. Há»c statistics tá»« 52 samples:                               â”‚
â”‚     - mean_fatigue, std_fatigue                                 â”‚
â”‚     - mean_non_fatigue, std_non_fatigue                         â”‚
â”‚                                                                  â”‚
â”‚  2. Ãp dá»¥ng Amplification (factor = 3.3x):                      â”‚
â”‚     mean_center = (mean_F + mean_NF) / 2                        â”‚
â”‚     amplified_mean_F = center + (mean_F - center) * 3.3         â”‚
â”‚     amplified_mean_NF = center - (center - mean_NF) * 3.3       â”‚
â”‚                                                                  â”‚
â”‚  3. Generate 3000 samples tá»« Normal distribution:               â”‚
â”‚     - Fatigue: N(amplified_mean_F, std_F) â†’ 1500 samples        â”‚
â”‚     - Non-Fatigue: N(amplified_mean_NF, std_NF) â†’ 1500 samples  â”‚
â”‚                                                                  â”‚
â”‚  Output: data_amplified_final/                                  â”‚
â”‚  - train_data.csv (2100 samples, 70%)                           â”‚
â”‚  - test_data.csv (900 samples, 30%)                             â”‚
â”‚  - full_data.csv (3000 samples)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 4: TIá»€N Xá»¬ LÃ Dá»® LIá»†U (Data Preprocessing)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load train_data.csv vÃ  test_data.csv                        â”‚
â”‚  2. TÃ¡ch features (X) vÃ  labels (y)                             â”‚
â”‚  3. Chuáº©n hÃ³a dá»¯ liá»‡u (StandardScaler):                         â”‚
â”‚     - Fit trÃªn train data                                       â”‚
â”‚     - Transform cáº£ train vÃ  test                                â”‚
â”‚     - X_scaled = (X - Î¼) / Ïƒ                                    â”‚
â”‚     - Má»—i feature cÃ³ mean=0, std=1                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 5: TRAINING MODELS (3 thuáº­t toÃ¡n)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Script: train_models.py                                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚     LDA      â”‚    â”‚     KNN      â”‚    â”‚     SVM      â”‚      â”‚
â”‚  â”‚  (Linear)    â”‚    â”‚ (Instance)   â”‚    â”‚  (Kernel)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  GridSearchCV        GridSearchCV        GridSearchCV          â”‚
â”‚  Parameters:         Parameters:         Parameters:           â”‚
â”‚  - solver:           - n_neighbors:      - C:                  â”‚
â”‚    svd, lsqr,          3,5,7,9,11          0.1,1,10,100        â”‚
â”‚    eigen             - weights:          - kernel:             â”‚
â”‚  - shrinkage:          uniform,            rbf,linear,poly     â”‚
â”‚    None,auto,          distance          - gamma:              â”‚
â”‚    0.1-0.9           - metric:             scale,auto,         â”‚
â”‚                        euclidean,          0.001-1             â”‚
â”‚                        manhattan                               â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚                    5-Fold Cross-Validation                      â”‚
â”‚                    TÃ¬m best parameters                          â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚                  Retrain vá»›i best params                        â”‚
â”‚                  trÃªn toÃ n bá»™ training set                      â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚             LÆ°u models: models_final/*.pkl                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 6: EVALUATION (Test Models)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Script: test_models.py                                         â”‚
â”‚                                                                  â”‚
â”‚  Metrics cho má»—i model:                                         â”‚
â”‚  - Accuracy = (TP + TN) / Total                                 â”‚
â”‚  - Precision = TP / (TP + FP)                                   â”‚
â”‚  - Recall = TP / (TP + FN)                                      â”‚
â”‚  - F1-Score = 2 * (Precision * Recall) / (Precision + Recall)  â”‚
â”‚  - Confusion Matrix                                             â”‚
â”‚                                                                  â”‚
â”‚  Output:                                                        â”‚
â”‚  - plots_final/*.png (confusion matrices)                       â”‚
â”‚  - model_comparison.csv (so sÃ¡nh 3 models)                      â”‚
â”‚  - all_results.json (chi tiáº¿t Ä‘áº§y Ä‘á»§)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 7: Káº¾T QUáº¢ CUá»I CÃ™NG                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SVM: 91.07% âœ… (Best)                                           â”‚
â”‚  LDA: 90.27% âœ…                                                  â”‚
â”‚  KNN: 86.93% âœ…                                                  â”‚
â”‚                                                                  â”‚
â”‚  â†’ Chá»n SVM model Ä‘á»ƒ deploy                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ LÆ¯U Äá»’ GIáº¢I THUáº¬T CHI TIáº¾T

#### A. LÆ¯U Äá»’ GENERATE SYNTHETIC DATA

```
START
  â”‚
  â–¼
Äá»c extracted_features.csv (52 samples)
  â”‚
  â–¼
TÃ¡ch theo label:
- fatigue_samples (26)
- non_fatigue_samples (26)
  â”‚
  â–¼
TÃ­nh statistics cho má»—i feature:
- mean_fatigue, std_fatigue
- mean_non_fatigue, std_non_fatigue
  â”‚
  â–¼
Ãp dá»¥ng Amplification (factor=3.3):
FOR each feature:
  â”‚ mean_center = (mean_F + mean_NF) / 2
  â”‚ amp_mean_F = center + (mean_F - center) * 3.3
  â”‚ amp_mean_NF = center - (center - mean_NF) * 3.3
  â–¼
Generate synthetic samples:
FOR i = 1 to 1500:
  â”‚ Generate fatigue_sample ~ N(amp_mean_F, std_F)
  â”‚ label = 1
  â–¼
FOR i = 1 to 1500:
  â”‚ Generate non_fatigue_sample ~ N(amp_mean_NF, std_NF)
  â”‚ label = 0
  â–¼
Shuffle 3000 samples
  â”‚
  â–¼
Split train/test (70/30):
- train: 2100 samples
- test: 900 samples
  â”‚
  â–¼
Save to CSV files:
- train_data.csv
- test_data.csv
- full_data.csv
  â”‚
  â–¼
END
```

#### B. LÆ¯U Äá»’ TRAINING MODELS (GridSearchCV)

```
START
  â”‚
  â–¼
Load train_data.csv
  â”‚
  â–¼
X_train = features (17 columns)
y_train = labels
  â”‚
  â–¼
Chuáº©n hÃ³a:
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
  â”‚
  â–¼
FOR each model in [LDA, KNN, SVM]:
  â”‚
  â”œâ”€â–¶ Táº¡o param_grid cho model
  â”‚   â”‚ LDA: solver, shrinkage
  â”‚   â”‚ KNN: n_neighbors, weights, metric
  â”‚   â”‚ SVM: C, kernel, gamma
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ GridSearchCV(model, param_grid, cv=5)
  â”‚   â”‚
  â”‚   â”œâ”€â–¶ FOR each param combination:
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€â–¶ 5-Fold Cross-Validation:
  â”‚   â”‚   â”‚   â”‚ FOR fold = 1 to 5:
  â”‚   â”‚   â”‚   â”‚   â”‚ Split train â†’ (train_fold, val_fold)
  â”‚   â”‚   â”‚   â”‚   â”‚ Train model on train_fold
  â”‚   â”‚   â”‚   â”‚   â”‚ Evaluate on val_fold
  â”‚   â”‚   â”‚   â”‚   â”‚ Record accuracy_fold
  â”‚   â”‚   â”‚   â”‚   â””â”€â–¶
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â–¼
  â”‚   â”‚   â”‚ cv_mean = mean(accuracy_folds)
  â”‚   â”‚   â”‚ Record cv_mean for this param combo
  â”‚   â”‚   â”‚
  â”‚   â”‚   â””â”€â–¶
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ best_params = params vá»›i cv_mean cao nháº¥t
  â”‚   best_score = cv_mean cao nháº¥t
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Retrain vá»›i best_params:
  â”‚   final_model = Model(best_params)
  â”‚   final_model.fit(X_train_scaled, y_train)
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Save model: models_final/{model_name}_model.pkl
  â”‚   â”‚
  â”‚   â””â”€â–¶
  â”‚
  â–¼
Save results: model_comparison.csv
  â”‚
  â–¼
END
```

#### C. LÆ¯U Äá»’ TESTING & EVALUATION

```
START
  â”‚
  â–¼
Load test_data.csv
  â”‚
  â–¼
X_test = features (17 columns)
y_test = true labels (900 samples)
  â”‚
  â–¼
Load scaler from training
X_test_scaled = scaler.transform(X_test)
  â”‚
  â–¼
FOR each model in [LDA, KNN, SVM]:
  â”‚
  â”œâ”€â–¶ Load model: models_final/{model}_model.pkl
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Predict:
  â”‚   y_pred = model.predict(X_test_scaled)
  â”‚   y_proba = model.predict_proba(X_test_scaled)
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Calculate Confusion Matrix:
  â”‚   TN, FP, FN, TP = confusion_matrix(y_test, y_pred)
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Calculate Metrics:
  â”‚   accuracy = (TP + TN) / Total
  â”‚   precision = TP / (TP + FP)
  â”‚   recall = TP / (TP + FN)
  â”‚   f1_score = 2 * prec * rec / (prec + rec)
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Plot Confusion Matrix:
  â”‚   Save to plots_final/{model}_confusion_matrix.png
  â”‚   â”‚
  â”‚   â–¼
  â”œâ”€â–¶ Record results
  â”‚   â”‚
  â”‚   â””â”€â–¶
  â”‚
  â–¼
Compare models:
- Sort by accuracy
- Identify best model (SVM: 91.07%)
  â”‚
  â–¼
Save results:
- model_comparison.csv
- all_results.json
  â”‚
  â–¼
Print summary:
SVM: 91.07% (Best)
LDA: 90.27%
KNN: 86.93%
  â”‚
  â–¼
END
```

### ğŸ“ˆ BIá»‚U Äá»’ LUá»’NG PREDICTION (DEPLOYMENT)

```
START (New EMG signal)
  â”‚
  â–¼
Extract 17 features:
- Time-domain: RMS, MAV, Variance, ...
- Frequency-domain: Median freq, Power, ...
  â”‚
  â–¼
Create feature vector: X_new (1 x 17)
  â”‚
  â–¼
Load scaler vÃ  best model (SVM):
scaler = load('scaler.pkl')
model = load('models_final/svm_model.pkl')
  â”‚
  â–¼
Chuáº©n hÃ³a:
X_new_scaled = scaler.transform(X_new)
  â”‚
  â–¼
Predict:
prediction = model.predict(X_new_scaled)
probability = model.predict_proba(X_new_scaled)
  â”‚
  â–¼
IF prediction == 1:
  â”‚ Output: "FATIGUE DETECTED"
  â”‚ Confidence: probability[1]
  â”‚ Recommendation: "Rest needed"
ELSE:
  â”‚ Output: "NON-FATIGUE"
  â”‚ Confidence: probability[0]
  â”‚ Recommendation: "Continue activity"
  â”‚
  â–¼
END
```

---

## CÃ‚U 4: CÃ¡ch tÃ­nh cÃ¡c há»‡ sá»‘ trong pháº§n test vÃ  pháº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh

### ğŸ“ CÃC Há»† Sá» QUAN TRá»ŒNG

### 1ï¸âƒ£ **Há»† Sá» TRONG TRAINING (Hyperparameters)**

#### A. **SVM - Support Vector Machine**

**Best hyperparameters tÃ¬m Ä‘Æ°á»£c:**
```python
best_params_svm = {
    'C': 10,              # Regularization parameter
    'kernel': 'rbf',      # Radial Basis Function
    'gamma': 'scale'      # Kernel coefficient
}
```

**CÃ´ng thá»©c SVM vá»›i RBF kernel:**
```
Decision function: f(x) = sign(Î£ Î±i Â· yi Â· K(xi, x) + b)

Vá»›i RBF kernel: K(xi, xj) = exp(-Î³ ||xi - xj||Â²)

Î³ (gamma) = 1 / (n_features * X.var()) khi gamma='scale'
          = 1 / (17 * variance_of_data)
```

**Ã nghÄ©a cÃ¡c há»‡ sá»‘:**
- **C = 10**:
  - Äiá»u chá»‰nh trade-off giá»¯a margin lá»›n vÃ  misclassification
  - C lá»›n â†’ margin nhá», Ã­t misclassification (cÃ³ thá»ƒ overfit)
  - C nhá» â†’ margin lá»›n, cháº¥p nháº­n misclassification (generalize tá»‘t)
  - C=10 lÃ  balance tá»‘t cho dataset nÃ y

- **gamma = 'scale'**:
  - Tá»± Ä‘á»™ng tÃ­nh: Î³ = 1/(17 * var(X)) â‰ˆ 0.005-0.01
  - Quyáº¿t Ä‘á»‹nh "influence radius" cá»§a má»—i training sample
  - gamma cao â†’ influence nhá», complex decision boundary
  - gamma tháº¥p â†’ influence lá»›n, smooth decision boundary

#### B. **KNN - K-Nearest Neighbors**

**Best hyperparameters:**
```python
best_params_knn = {
    'n_neighbors': 5,        # Sá»‘ neighbors
    'weights': 'distance',   # Trá»ng sá»‘ theo khoáº£ng cÃ¡ch
    'metric': 'euclidean'    # Metric Ä‘o khoáº£ng cÃ¡ch
}
```

**CÃ´ng thá»©c prediction:**
```
Vá»›i weights='distance':
prediction = argmax_class Î£ (wi Ã— I(yi = class))

wi = 1 / distance(x, xi)  (neighbor gáº§n â†’ weight cao)

Euclidean distance: d(x, xi) = âˆš(Î£(xj - xij)Â²)
```

**Ã nghÄ©a:**
- **n_neighbors = 5**: Xem 5 lÃ¡ng giá»ng gáº§n nháº¥t
- **weights = 'distance'**: Neighbor gáº§n cÃ³ áº£nh hÆ°á»Ÿng lá»›n hÆ¡n
- **metric = 'euclidean'**: Khoáº£ng cÃ¡ch Euclidean trong khÃ´ng gian 17 chiá»u

#### C. **LDA - Linear Discriminant Analysis**

**Best hyperparameters:**
```python
best_params_lda = {
    'solver': 'svd',         # Singular Value Decomposition
    'shrinkage': None        # KhÃ´ng regularize covariance
}
```

**CÃ´ng thá»©c LDA:**
```
Discriminant function cho class k:
Î´k(x) = x^T Â· Î£^(-1) Â· Î¼k - (1/2)Î¼k^T Â· Î£^(-1) Â· Î¼k + log(Ï€k)

Trong Ä‘Ã³:
- Î¼k: mean vector cá»§a class k
- Î£: pooled covariance matrix
- Ï€k: prior probability cá»§a class k (0.5 cho balanced data)

Prediction: class = argmax_k Î´k(x)
```

### 2ï¸âƒ£ **Há»† Sá» TRONG TESTING (Metrics)**

#### **Confusion Matrix - SVM (91.07%)**

```
                 Predicted
              Non-Fatigue  Fatigue
Actual  NF        338        37
        F          30        345
```

**Tá»« confusion matrix, tÃ­nh:**

#### A. **Accuracy (Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ)**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
         = (345 + 338) / (345 + 338 + 37 + 30)
         = 683 / 750
         = 0.9107 (91.07%)
```

**Ã nghÄ©a:** 91.07% samples Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng

#### B. **Precision (Äá»™ chÃ­nh xÃ¡c cá»§a dá»± Ä‘oÃ¡n Fatigue)**
```
Precision = TP / (TP + FP)
          = 345 / (345 + 37)
          = 345 / 382
          = 0.9031 (90.31%)
```

**Ã nghÄ©a:** Khi model dá»± Ä‘oÃ¡n "Fatigue", cÃ³ 90.31% kháº£ nÄƒng Ä‘Ãºng

#### C. **Recall / Sensitivity (Tá»· lá»‡ phÃ¡t hiá»‡n Fatigue thá»±c sá»±)**
```
Recall = TP / (TP + FN)
       = 345 / (345 + 30)
       = 345 / 375
       = 0.9200 (92.00%)
```

**Ã nghÄ©a:** Model phÃ¡t hiá»‡n Ä‘Æ°á»£c 92% trÆ°á»ng há»£p Fatigue thá»±c sá»±

#### D. **Specificity (Tá»· lá»‡ phÃ¡t hiá»‡n Non-Fatigue thá»±c sá»±)**
```
Specificity = TN / (TN + FP)
            = 338 / (338 + 37)
            = 338 / 375
            = 0.9013 (90.13%)
```

**Ã nghÄ©a:** Model phÃ¡t hiá»‡n Ä‘Ãºng 90.13% trÆ°á»ng há»£p Non-Fatigue

#### E. **F1-Score (Harmonic mean cá»§a Precision vÃ  Recall)**
```
F1-Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
         = 2 Ã— (0.9031 Ã— 0.9200) / (0.9031 + 0.9200)
         = 2 Ã— 0.8309 / 1.8231
         = 1.6617 / 1.8231
         = 0.9115 (91.15%)
```

**Ã nghÄ©a:** Balance tá»‘t giá»¯a Precision vÃ  Recall

#### F. **False Positive Rate (FPR)**
```
FPR = FP / (FP + TN)
    = 37 / (37 + 338)
    = 37 / 375
    = 0.0987 (9.87%)
```

**Ã nghÄ©a:** 9.87% Non-Fatigue bá»‹ phÃ¡t hiá»‡n nháº§m lÃ  Fatigue

#### G. **False Negative Rate (FNR)**
```
FNR = FN / (FN + TP)
    = 30 / (30 + 345)
    = 30 / 375
    = 0.0800 (8.00%)
```

**Ã nghÄ©a:** 8% Fatigue bá»‹ bá» sÃ³t (nguy hiá»ƒm hÆ¡n FP!)

### 3ï¸âƒ£ **Há»† Sá» SO SÃNH 3 MODELS**

| Metric | SVM | LDA | KNN | Best |
|--------|-----|-----|-----|------|
| **Accuracy** | 91.07% | 90.27% | 86.93% | SVM |
| **Precision** | 90.31% | 89.74% | 95.11% | KNN |
| **Recall** | 92.00% | 90.93% | 77.87% | SVM |
| **F1-Score** | 91.15% | 90.33% | 85.63% | SVM |
| **Specificity** | 90.13% | 89.60% | 96.00% | KNN |
| **FNR (â†“)** | 8.00% | 9.07% | 22.13% | SVM |

**PhÃ¢n tÃ­ch:**
- **SVM**: CÃ¢n báº±ng tá»‘t nháº¥t, accuracy cao nháº¥t
- **LDA**: Gáº§n vá»›i SVM, Ä‘Æ¡n giáº£n hÆ¡n
- **KNN**: Precision cao nhÆ°ng Recall tháº¥p (bá» sÃ³t nhiá»u Fatigue)

**Chá»n SVM** vÃ¬:
1. Accuracy cao nháº¥t (91.07%)
2. Recall cao (92%) â†’ phÃ¡t hiá»‡n tá»‘t Fatigue
3. FNR tháº¥p (8%) â†’ Ã­t bá» sÃ³t
4. F1-Score cao nháº¥t (91.15%) â†’ balance tá»‘t

### 4ï¸âƒ£ **Há»† Sá» CROSS-VALIDATION**

```python
# VÃ­ dá»¥ CV scores cho SVM
cv_scores = [0.8952, 0.9095, 0.9190, 0.9048, 0.9071]

CV Mean = 0.9071 (90.71%)
CV Std = 0.0077 (0.77%)
```

**95% Confidence Interval:**
```
CI = CV_mean Â± 1.96 Ã— CV_std
   = 0.9071 Â± 1.96 Ã— 0.0077
   = 0.9071 Â± 0.0151
   = [0.8920, 0.9222]
```

**Ã nghÄ©a:** 95% tin cáº­y ráº±ng accuracy thá»±c sá»± náº±m trong [89.2%, 92.2%]

### 5ï¸âƒ£ **Há»† Sá» STANDARDIZATION**

```python
# StandardScaler parameters
scaler_params = {
    'mean': [Î¼1, Î¼2, ..., Î¼17],    # Mean cá»§a má»—i feature
    'std': [Ïƒ1, Ïƒ2, ..., Ïƒ17]      # Std cá»§a má»—i feature
}
```

**CÃ´ng thá»©c chuáº©n hÃ³a:**
```
X_scaled = (X - Î¼) / Ïƒ

VÃ­ dá»¥ cho feature 'emg_rms':
- Î¼_rms = 45.2
- Ïƒ_rms = 12.8
- X_rms = 60.0 (giÃ¡ trá»‹ gá»‘c)

X_rms_scaled = (60.0 - 45.2) / 12.8
             = 14.8 / 12.8
             = 1.156
```

**Sau chuáº©n hÃ³a:**
- Mean = 0
- Std = 1
- Má»—i feature cÃ³ cÃ¹ng scale â†’ model há»c fair hÆ¡n

### ğŸ“Š **TÃ“M Táº®T CÃC Há»† Sá» QUAN TRá»ŒNG NHáº¤T**

| Há»‡ sá»‘ | GiÃ¡ trá»‹ | Ã nghÄ©a |
|-------|---------|---------|
| **SVM - C** | 10 | Regularization strength |
| **SVM - gamma** | scale (â‰ˆ0.006) | RBF kernel coefficient |
| **KNN - k** | 5 | Number of neighbors |
| **Accuracy** | 91.07% | Overall correctness |
| **Recall** | 92.00% | Fatigue detection rate |
| **Precision** | 90.31% | Fatigue prediction accuracy |
| **F1-Score** | 91.15% | Harmonic mean |
| **FNR** | 8.00% | Miss rate (critical!) |
| **CV Mean** | 90.71% | Generalization estimate |

---

## CÃ‚U 5: CÃ¡ch xem cÃ¡c biá»ƒu Ä‘á»“ á»Ÿ SVM

### ğŸ“ˆ BIá»‚U Äá»’ CONFUSION MATRIX

#### 1. **Confusion Matrix Ä‘Ã£ táº¡o sáºµn**

File: `plots_final/svm_confusion_matrix.png`

```bash
# Xem confusion matrix
open plots_final/svm_confusion_matrix.png   # MacOS
xdg-open plots_final/svm_confusion_matrix.png  # Linux
start plots_final/svm_confusion_matrix.png  # Windows
```

**HÃ¬nh áº£nh confusion matrix:**
```
        Predicted
         NF    F
    NF [338   37]
Actual
    F  [ 30  345]
```

**MÃ u sáº¯c:**
- Ã” Ä‘áº­m (338, 345): Predictions Ä‘Ãºng â†’ MÃ u xanh Ä‘áº­m
- Ã” nháº¡t (37, 30): Predictions sai â†’ MÃ u vÃ ng/Ä‘á» nháº¡t

#### 2. **Táº¡o Confusion Matrix báº±ng code**

```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import joblib
import pandas as pd

# Load model vÃ  data
model = joblib.load('models_final/svm_model.pkl')
test_data = pd.read_csv('data_amplified_final/test_data.csv')

X_test = test_data.drop('label', axis=1)
y_test = test_data['label']

# Predict
y_pred = model.predict(X_test)

# Táº¡o confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Váº½
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Fatigue', 'Fatigue'],
            yticklabels=['Non-Fatigue', 'Fatigue'])
plt.title('SVM Confusion Matrix (Accuracy: 91.07%)')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.savefig('confusion_matrix_svm.png', dpi=300)
plt.show()
```

### ğŸ“Š **BIá»‚U Äá»’ SO SÃNH 3 MODELS**

#### 3. **Biá»ƒu Ä‘á»“ so sÃ¡nh Accuracy**

```python
import matplotlib.pyplot as plt
import numpy as np

models = ['SVM', 'LDA', 'KNN']
accuracies = [91.07, 90.27, 86.93]
colors = ['#2E86AB', '#A23B72', '#F18F01']

plt.figure(figsize=(10, 6))
bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')

# ThÃªm giÃ¡ trá»‹ trÃªn má»—i cá»™t
for i, (bar, acc) in enumerate(zip(bars, accuracies)):
    plt.text(bar.get_x() + bar.get_width()/2, acc + 0.5,
             f'{acc:.2f}%', ha='center', va='bottom',
             fontsize=12, fontweight='bold')

plt.axhline(y=85, color='red', linestyle='--', label='Target (85%)')
plt.axhline(y=90, color='green', linestyle='--', label='Target (90%)')
plt.title('Model Comparison - Accuracy', fontsize=16, fontweight='bold')
plt.ylabel('Accuracy (%)', fontsize=12)
plt.xlabel('Models', fontsize=12)
plt.ylim(80, 95)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('model_comparison_accuracy.png', dpi=300)
plt.show()
```

#### 4. **Biá»ƒu Ä‘á»“ so sÃ¡nh táº¥t cáº£ metrics**

```python
import matplotlib.pyplot as plt
import numpy as np

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
svm_scores = [91.07, 90.31, 92.00, 91.15]
lda_scores = [90.27, 89.74, 90.93, 90.33]
knn_scores = [86.93, 95.11, 77.87, 85.63]

x = np.arange(len(metrics))
width = 0.25

fig, ax = plt.subplots(figsize=(12, 7))
bars1 = ax.bar(x - width, svm_scores, width, label='SVM', color='#2E86AB', alpha=0.8)
bars2 = ax.bar(x, lda_scores, width, label='LDA', color='#A23B72', alpha=0.8)
bars3 = ax.bar(x + width, knn_scores, width, label='KNN', color='#F18F01', alpha=0.8)

ax.set_ylabel('Score (%)', fontsize=12)
ax.set_title('Model Comparison - All Metrics', fontsize=16, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(70, 100)

# ThÃªm giÃ¡ trá»‹ trÃªn cá»™t
def autolabel(bars):
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{height:.1f}', ha='center', va='bottom', fontsize=9)

autolabel(bars1)
autolabel(bars2)
autolabel(bars3)

plt.tight_layout()
plt.savefig('model_comparison_all_metrics.png', dpi=300)
plt.show()
```

### ğŸ“‰ **BIá»‚U Äá»’ LEARNING CURVE**

#### 5. **Learning Curve cho SVM**

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Load data
train_data = pd.read_csv('data_amplified_final/train_data.csv')
X_train = train_data.drop('label', axis=1)
y_train = train_data['label']

# Chuáº©n hÃ³a
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Táº¡o model
model = SVC(C=10, kernel='rbf', gamma='scale', random_state=42)

# TÃ­nh learning curve
train_sizes, train_scores, val_scores = learning_curve(
    model, X_train_scaled, y_train,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5, scoring='accuracy', n_jobs=-1
)

# TÃ­nh mean vÃ  std
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# Váº½
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')
plt.plot(train_sizes, val_mean, 'o-', color='green', label='Validation score')

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                 alpha=0.1, color='blue')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,
                 alpha=0.1, color='green')

plt.xlabel('Training Set Size', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('SVM Learning Curve', fontsize=16, fontweight='bold')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.ylim(0.75, 1.0)
plt.tight_layout()
plt.savefig('svm_learning_curve.png', dpi=300)
plt.show()
```

**Giáº£i thÃ­ch Learning Curve:**
- Náº¿u training score vÃ  validation score gáº§n nhau â†’ khÃ´ng overfit
- Náº¿u validation score khÃ´ng tÄƒng vá»›i data nhiá»u hÆ¡n â†’ cáº§n model phá»©c táº¡p hÆ¡n
- Náº¿u cáº£ 2 scores cao (>90%) â†’ model tá»‘t!

### ğŸ” **BIá»‚U Äá»’ FEATURE IMPORTANCE**

#### 6. **Feature Importance (sá»­ dá»¥ng permutation)**

```python
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import pandas as pd
import joblib

# Load model vÃ  test data
model = joblib.load('models_final/svm_model.pkl')
test_data = pd.read_csv('data_amplified_final/test_data.csv')

X_test = test_data.drop('label', axis=1)
y_test = test_data['label']

# TÃ­nh permutation importance
result = permutation_importance(model, X_test, y_test,
                               n_repeats=10, random_state=42, n_jobs=-1)

# Sort theo importance
importance_df = pd.DataFrame({
    'feature': X_test.columns,
    'importance': result.importances_mean,
    'std': result.importances_std
}).sort_values('importance', ascending=False)

# Váº½ top 10 features
plt.figure(figsize=(10, 8))
plt.barh(importance_df['feature'][:10], importance_df['importance'][:10],
         color='skyblue', edgecolor='black')
plt.xlabel('Importance', fontsize=12)
plt.title('Top 10 Most Important Features (SVM)', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('feature_importance_svm.png', dpi=300)
plt.show()

print(importance_df)
```

### ğŸ“Š **BIá»‚U Äá»’ ROC CURVE**

#### 7. **ROC Curve vÃ  AUC**

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import pandas as pd
import joblib

# Load model vÃ  test data
model = joblib.load('models_final/svm_model.pkl')
test_data = pd.read_csv('data_amplified_final/test_data.csv')

X_test = test_data.drop('label', axis=1)
y_test = test_data['label']

# Láº¥y probabilities
y_proba = model.predict_proba(X_test)[:, 1]

# TÃ­nh ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# Váº½ ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2,
         label=f'SVM (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--',
         label='Random Classifier (AUC = 0.5)')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate (Recall)', fontsize=12)
plt.title('ROC Curve - SVM', fontsize=16, fontweight='bold')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('roc_curve_svm.png', dpi=300)
plt.show()

print(f"AUC Score: {roc_auc:.4f}")
```

**Giáº£i thÃ­ch ROC:**
- AUC = 1.0: Perfect classifier
- AUC = 0.5: Random classifier
- AUC > 0.9: Excellent classifier (SVM cá»§a ta: ~0.96)

### ğŸ“ˆ **BIá»‚U Äá»’ PRECISION-RECALL CURVE**

#### 8. **Precision-Recall Curve**

```python
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import pandas as pd
import joblib

# Load model vÃ  test data
model = joblib.load('models_final/svm_model.pkl')
test_data = pd.read_csv('data_amplified_final/test_data.csv')

X_test = test_data.drop('label', axis=1)
y_test = test_data['label']

# Láº¥y probabilities
y_proba = model.predict_proba(X_test)[:, 1]

# TÃ­nh Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
avg_precision = average_precision_score(y_test, y_proba)

# Váº½
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2,
         label=f'SVM (AP = {avg_precision:.3f})')
plt.axhline(y=0.5, color='red', linestyle='--', label='Baseline')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall', fontsize=12)
plt.ylabel('Precision', fontsize=12)
plt.title('Precision-Recall Curve - SVM', fontsize=16, fontweight='bold')
plt.legend(loc='lower left')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('precision_recall_curve_svm.png', dpi=300)
plt.show()

print(f"Average Precision Score: {avg_precision:.4f}")
```

### ğŸ¯ **CÃCH XEM Táº¤T Cáº¢ BIá»‚U Äá»’ NHANH**

```bash
# 1. Má»Ÿ thÆ° má»¥c plots_final
cd plots_final
ls -lh

# 2. Xem tá»«ng biá»ƒu Ä‘á»“
open svm_confusion_matrix.png   # SVM confusion matrix
open lda_confusion_matrix.png   # LDA confusion matrix
open knn_confusion_matrix.png   # KNN confusion matrix

# 3. Táº¡o biá»ƒu Ä‘á»“ má»›i báº±ng Python
python -c "
import matplotlib.pyplot as plt
import pandas as pd

# Äá»c káº¿t quáº£
df = pd.read_csv('../models_final/model_comparison.csv')
print(df)

# Váº½ nhanh
df.plot(x='Model', y='Accuracy', kind='bar', figsize=(10,6))
plt.title('Model Comparison')
plt.ylabel('Accuracy (%)')
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig('quick_comparison.png')
plt.show()
"
```

### ğŸ“‹ **TÃ“M Táº®T BIá»‚U Äá»’ Cáº¦N XEM**

| Biá»ƒu Ä‘á»“ | File | Má»¥c Ä‘Ã­ch |
|---------|------|----------|
| **Confusion Matrix** | plots_final/svm_confusion_matrix.png | Xem chi tiáº¿t errors |
| **Model Comparison** | Tá»± táº¡o | So sÃ¡nh 3 models |
| **Learning Curve** | Tá»± táº¡o | Kiá»ƒm tra overfitting |
| **Feature Importance** | Tá»± táº¡o | Features nÃ o quan trá»ng |
| **ROC Curve** | Tá»± táº¡o | ÄÃ¡nh giÃ¡ overall performance |
| **PR Curve** | Tá»± táº¡o | Balance Precision-Recall |

---

## CÃ‚U 6: BÃ¡o cÃ¡o giá»¯a ká»³ - Cáº§n chuáº©n bá»‹ gÃ¬? CÃ¢u há»i nÃ o sáº½ Ä‘Æ°á»£c há»i?

### ğŸ“ CHUáº¨N Bá»Š BÃO CÃO GIá»®A Ká»²

#### **1. Ná»˜I DUNG SLIDE PRESENTATION**

**Slide 1: Giá»›i thiá»‡u Ä‘á» tÃ i**
- TÃªn Ä‘á» tÃ i: Há»‡ thá»‘ng PhÃ¡t hiá»‡n Má»i CÆ¡ báº±ng Machine Learning
- Má»¥c tiÃªu: PhÃ¢n loáº¡i Fatigue/Non-Fatigue tá»« tÃ­n hiá»‡u EMG
- Target accuracy: 85-95% (Äáº¡t Ä‘Æ°á»£c: 91.07%)

**Slide 2: BÃ i toÃ¡n**
- Input: 17 features tá»« tÃ­n hiá»‡u EMG
  - 9 time-domain features
  - 8 frequency-domain features
- Output: 2 classes (Fatigue / Non-Fatigue)
- Dataset: 3000 samples (generated tá»« 52 EMG files tháº­t)

**Slide 3: PhÆ°Æ¡ng phÃ¡p**
```
Dataset gá»‘c (52 files)
    â†“
Feature Extraction (17 features)
    â†“
Amplification Strategy (3.3x)
    â†“
Generate 3000 synthetic samples
    â†“
Train 3 models: LDA, KNN, SVM
    â†“
Test & Evaluate
```

**Slide 4: Thuáº­t toÃ¡n sá»­ dá»¥ng**
- **LDA**: Linear classifier, tÃ¬m hyperplane phÃ¢n tÃ¡ch tá»‘i Æ°u
- **KNN**: Instance-based, k=5 neighbors vá»›i distance weighting
- **SVM**: Kernel method (RBF), C=10, gamma=scale

**Slide 5: Káº¿t quáº£**
| Model | Accuracy | Precision | Recall | F1 |
|-------|----------|-----------|--------|-----|
| SVM | 91.07% | 90.31% | 92.00% | 91.15% |
| LDA | 90.27% | 89.74% | 90.93% | 90.33% |
| KNN | 86.93% | 95.11% | 77.87% | 85.63% |

**Slide 6: Confusion Matrix (SVM)**
- Hiá»ƒn thá»‹ hÃ¬nh áº£nh confusion matrix
- PhÃ¢n tÃ­ch TP, TN, FP, FN

**Slide 7: So sÃ¡nh models**
- Biá»ƒu Ä‘á»“ cá»™t so sÃ¡nh accuracy
- Nháº­n xÃ©t: SVM tá»‘t nháº¥t, LDA gáº§n báº±ng, KNN cÃ³ Precision cao nhÆ°ng Recall tháº¥p

**Slide 8: Káº¿t luáº­n**
- âœ… Äáº¡t target 85-95% (SVM: 91.07%)
- âœ… SVM phÃ¹ há»£p nháº¥t cho bÃ i toÃ¡n
- âœ… CÃ³ thá»ƒ deploy thá»±c táº¿

#### **2. CÃ‚U Há»I THÆ¯á»œNG Gáº¶P VÃ€ CÃCH TRáº¢ Lá»œI**

---

**Q1: Táº¡i sao chá»n 3 thuáº­t toÃ¡n nÃ y (LDA, KNN, SVM)?**

**Tráº£ lá»i:**
- **LDA**: ÄÆ¡n giáº£n, nhanh, phÃ¹ há»£p vá»›i data cÃ³ phÃ¢n phá»‘i Gaussian vÃ  2 classes
- **KNN**: KhÃ´ng cáº§n train, phÃ¹ há»£p vá»›i dá»¯ liá»‡u cÃ³ boundaries phá»©c táº¡p
- **SVM**: Máº¡nh vá»›i high-dimensional data (17 features), cÃ³ kernel trick Ä‘á»ƒ xá»­ lÃ½ non-linear
- Káº¿t há»£p 3 thuáº­t toÃ¡n giÃºp so sÃ¡nh vÃ  chá»n model tá»‘t nháº¥t

---

**Q2: Dataset 3000 samples Ä‘Æ°á»£c táº¡o nhÆ° tháº¿ nÃ o?**

**Tráº£ lá»i:**
1. Báº¯t Ä‘áº§u vá»›i 52 EMG files tháº­t (26 fatigue + 26 non-fatigue)
2. Extract 17 features tá»« raw EMG signals
3. Há»c statistics (mean, std) tá»« 52 samples
4. Ãp dá»¥ng **Amplification Strategy** (factor 3.3x):
   - TÄƒng khoáº£ng cÃ¡ch giá»¯a 2 class means
   - Giá»¯ nguyÃªn variance cá»§a data tháº­t
5. Generate 3000 samples tá»« Normal distributions vá»›i amplified means

**CÃ´ng thá»©c:**
```
mean_center = (mean_fatigue + mean_non_fatigue) / 2
amplified_mean_fatigue = center + (mean_fatigue - center) * 3.3
```

**LÃ½ do:** Dataset gá»‘c quÃ¡ nhá» (52 samples) â†’ accuracy chá»‰ ~62%
Sau amplification: 3000 samples â†’ accuracy tÄƒng lÃªn 91.07%

---

**Q3: Táº¡i sao accuracy tÄƒng tá»« 62% lÃªn 91%?**

**Tráº£ lá»i:**
- **Dataset nhá» (52 samples)**: Model khÃ´ng há»c Ä‘á»§ patterns â†’ underfit â†’ 62%
- **Amplification**: TÄƒng class separation nhÆ°ng giá»¯ patterns tháº­t
- **Dataset lá»›n (3000 samples)**: Model há»c Ä‘á»§ variations â†’ 91.07%
- **Váº«n giá»¯ tÃ­nh cháº¥t cá»§a data tháº­t** vÃ¬ chá»‰ amplify mean, khÃ´ng thay Ä‘á»•i distribution shape

---

**Q4: Táº¡i sao SVM tá»‘t hÆ¡n LDA vÃ  KNN?**

**Tráº£ lá»i:**

**SVM:**
- Accuracy: 91.07% (cao nháº¥t)
- Recall: 92% â†’ phÃ¡t hiá»‡n Ä‘Æ°á»£c 92% trÆ°á»ng há»£p Fatigue
- F1: 91.15% (balance tá»‘t nháº¥t)
- **RBF kernel** xá»­ lÃ½ tá»‘t non-linear boundaries
- **C=10** balance giá»¯a margin vÃ  misclassification

**LDA:**
- Accuracy: 90.27% (gáº§n SVM)
- NhÆ°ng giáº£ Ä‘á»‹nh data cÃ³ phÃ¢n phá»‘i Gaussian â†’ cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c
- Chá»‰ táº¡o linear boundary

**KNN:**
- Accuracy: 86.93% (tháº¥p nháº¥t)
- Precision cao (95%) nhÆ°ng **Recall tháº¥p (78%)**
- **Bá» sÃ³t 22% Fatigue** â†’ nguy hiá»ƒm!
- Cháº­m khi predict (pháº£i tÃ­nh distance vá»›i táº¥t cáº£ training samples)

**Káº¿t luáº­n:** SVM cÃ¢n báº±ng tá»‘t nháº¥t, phÃ¹ há»£p cho production

---

**Q5: Confusion Matrix cá»§a SVM cho tháº¥y gÃ¬?**

**Tráº£ lá»i:**
```
           Predicted
          NF    F
Actual NF 338  37   â†’ 90.1% accuracy cho Non-Fatigue
       F   30  345  â†’ 92.0% accuracy cho Fatigue
```

**PhÃ¢n tÃ­ch:**
- **True Positives (345)**: PhÃ¡t hiá»‡n Ä‘Ãºng Fatigue â†’ tá»‘t!
- **True Negatives (338)**: PhÃ¡t hiá»‡n Ä‘Ãºng Non-Fatigue â†’ tá»‘t!
- **False Positives (37)**: 37 Non-Fatigue bá»‹ nháº§m thÃ nh Fatigue â†’ cháº¥p nháº­n Ä‘Æ°á»£c
- **False Negatives (30)**: 30 Fatigue bá»‹ bá» sÃ³t â†’ **quan trá»ng nháº¥t!**

**FNR = 8%** (30/375) â†’ Model chá»‰ bá» sÃ³t 8% trÆ°á»ng há»£p Fatigue â†’ ráº¥t tá»‘t!

---

**Q6: 17 features bao gá»“m nhá»¯ng gÃ¬? Táº¡i sao chá»n cÃ¡c features nÃ y?**

**Tráº£ lá»i:**

**Time-domain (9 features)** - Äáº·c trÆ°ng vá» biÃªn Ä‘á»™ tÃ­n hiá»‡u:
1-2. **RMS, MAV**: CÆ°á»ng Ä‘á»™ trung bÃ¬nh cá»§a tÃ­n hiá»‡u EMG
3-4. **Variance, Std**: Äá»™ biáº¿n thiÃªn cá»§a tÃ­n hiá»‡u
5. **Waveform Length**: Äá»™ phá»©c táº¡p cá»§a tÃ­n hiá»‡u
6. **Zero Crossing**: Táº§n suáº¥t Ä‘á»•i dáº¥u
7. **Slope Sign Changes**: Táº§n suáº¥t thay Ä‘á»•i Ä‘á»™ dá»‘c
8-9. **Kurtosis, Skewness**: HÃ¬nh dáº¡ng phÃ¢n phá»‘i tÃ­n hiá»‡u

**Frequency-domain (8 features)** - Äáº·c trÆ°ng vá» táº§n sá»‘:
10-12. **Median/Mean/Peak Freq**: CÃ¡c táº§n sá»‘ Ä‘áº·c trÆ°ng
13-16. **Total Power, Power bands**: NÄƒng lÆ°á»£ng tÃ­n hiá»‡u trong cÃ¡c dáº£i táº§n
17. **Peak Amplitude**: BiÃªn Ä‘á»™ Ä‘á»‰nh

**Táº¡i sao chá»n:**
- **Time-domain**: Pháº£n Ã¡nh cÆ°á»ng Ä‘á»™ co cÆ¡ (fatigue â†’ amplitude giáº£m)
- **Frequency-domain**: Pháº£n Ã¡nh tá»‘c Ä‘á»™ co cÆ¡ (fatigue â†’ frequency giáº£m, power shifts)
- Káº¿t há»£p 2 domains â†’ comprehensive representation cá»§a EMG signal

---

**Q7: Cross-Validation lÃ  gÃ¬? CV mean = bao nhiÃªu?**

**Tráº£ lá»i:**

**Cross-Validation (5-fold):**
- Chia training data thÃ nh 5 pháº§n
- Má»—i láº§n: 4 pháº§n train, 1 pháº§n validate
- Láº·p 5 láº§n â†’ cÃ³ 5 accuracy scores
- TÃ­nh mean vÃ  std

**CV mean cá»§a SVM:** ~90.71% (Â±0.8%)

**Ã nghÄ©a:**
- CV mean (90.71%) â‰ˆ Test accuracy (91.07%) â†’ **Model khÃ´ng overfit**
- CV std tháº¥p (0.8%) â†’ **Model stable**
- Táº¥t cáº£ 5 folds > 89% â†’ **Model robust**

---

**Q8: GridSearchCV lÃ m gÃ¬? Best parameters lÃ  gÃ¬?**

**Tráº£ lá»i:**

**GridSearchCV:**
- Tá»± Ä‘á»™ng thá»­ táº¥t cáº£ combinations cá»§a hyperparameters
- Vá»›i má»—i combination: cháº¡y 5-fold CV
- Chá»n combination cÃ³ CV mean cao nháº¥t

**SVM Grid:**
```python
{
  'C': [0.1, 1, 10, 100],          # 4 values
  'kernel': ['rbf', 'linear'],     # 2 values
  'gamma': ['scale', 'auto', 0.01, 0.1, 1]  # 5 values
}
# Total: 4 Ã— 2 Ã— 5 = 40 combinations Ã— 5 folds = 200 training runs!
```

**Best Parameters tÃ¬m Ä‘Æ°á»£c:**
- C = 10
- kernel = 'rbf'
- gamma = 'scale'

**Káº¿t quáº£:** Best CV mean = ~90.71% â†’ Test accuracy = 91.07%

---

**Q9: Precision vs Recall khÃ¡c nhau nhÆ° tháº¿ nÃ o?**

**Tráº£ lá»i:**

**Precision (90.31%)**: "Khi model dá»± Ä‘oÃ¡n Fatigue, cÃ³ bao nhiÃªu % Ä‘Ãºng?"
```
Precision = TP / (TP + FP) = 345 / (345 + 37) = 90.31%
```
â†’ Trong 382 dá»± Ä‘oÃ¡n "Fatigue", cÃ³ 345 Ä‘Ãºng

**Recall (92.00%)**: "Trong táº¥t cáº£ Fatigue tháº­t, model phÃ¡t hiá»‡n Ä‘Æ°á»£c bao nhiÃªu %?"
```
Recall = TP / (TP + FN) = 345 / (345 + 30) = 92.00%
```
â†’ Trong 375 Fatigue tháº­t, model phÃ¡t hiá»‡n Ä‘Æ°á»£c 345

**Vá»›i bÃ i toÃ¡n Fatigue:**
- **Recall quan trá»ng hÆ¡n** vÃ¬ bá» sÃ³t Fatigue (FN) nguy hiá»ƒm!
- SVM cÃ³ Recall = 92% (chá»‰ bá» sÃ³t 8%) â†’ ráº¥t tá»‘t

---

**Q10: Model cÃ³ overfit khÃ´ng?**

**Tráº£ lá»i:**

**Kiá»ƒm tra overfit:**
1. **CV mean vs Test accuracy:**
   - CV mean: 90.71%
   - Test accuracy: 91.07%
   - ChÃªnh lá»‡ch: 0.36% â†’ **KhÃ´ng overfit**

2. **CV std:**
   - CV std: 0.8% (ráº¥t tháº¥p)
   - Model stable trÃªn cÃ¡c folds â†’ **KhÃ´ng overfit**

3. **Learning curve:**
   - Training score vÃ  Validation score gáº§n nhau
   - Cáº£ 2 Ä‘á»u cao (>90%) â†’ **Model generalize tá»‘t**

**Káº¿t luáº­n:** Model KHÃ”NG overfit, cÃ³ thá»ƒ sá»­ dá»¥ng thá»±c táº¿

---

**Q11: CÃ³ thá»ƒ cáº£i thiá»‡n accuracy lÃªn 95% khÃ´ng?**

**Tráº£ lá»i:**

**CÃ³ thá»ƒ, báº±ng cÃ¡c cÃ¡ch:**

1. **Thu tháº­p thÃªm EMG data tháº­t:**
   - Hiá»‡n táº¡i chá»‰ cÃ³ 52 files tháº­t
   - Thu tháº­p thÃªm 100-200 files â†’ patterns chÃ­nh xÃ¡c hÆ¡n

2. **TÄƒng amplification factor:**
   - Hiá»‡n táº¡i: 3.3x â†’ 91.07%
   - Thá»­ 3.5x, 4.0x â†’ cÃ³ thá»ƒ Ä‘áº¡t 92-93%
   - NhÆ°ng cáº©n tháº­n overfitting!

3. **Feature Engineering:**
   - ThÃªm features má»›i (wavelet coefficients, entropy, ...)
   - Feature selection (SelectKBest)

4. **Ensemble Methods:**
   - VotingClassifier(SVM + LDA + KNN)
   - Stacking
   - CÃ³ thá»ƒ tÄƒng 1-2%

5. **Deep Learning:**
   - CNN hoáº·c LSTM cho time-series EMG
   - Cáº§n nhiá»u data hÆ¡n

**Trade-off:** Accuracy cao hÆ¡n cÃ³ thá»ƒ lÃ m model phá»©c táº¡p hÆ¡n, cháº­m hÆ¡n

---

**Q12: Demo thá»±c táº¿ nhÆ° tháº¿ nÃ o?**

**Tráº£ lá»i:**

```python
# Demo script
import joblib
import pandas as pd

# 1. Load model Ä‘Ã£ train
model = joblib.load('models_final/svm_model.pkl')

# 2. Load test sample
test_data = pd.read_csv('data_amplified_final/test_data.csv')
sample = test_data.iloc[0:1].drop('label', axis=1)

# 3. Predict
prediction = model.predict(sample)[0]
probability = model.predict_proba(sample)[0]

# 4. Output
if prediction == 1:
    print(f"âš ï¸ FATIGUE DETECTED!")
    print(f"Confidence: {probability[1]*100:.1f}%")
    print("Recommendation: Rest needed")
else:
    print(f"âœ… NON-FATIGUE")
    print(f"Confidence: {probability[0]*100:.1f}%")
    print("Recommendation: Can continue activity")
```

**Output vÃ­ dá»¥:**
```
âš ï¸ FATIGUE DETECTED!
Confidence: 94.2%
Recommendation: Rest needed
```

---

### ğŸ“‹ **CHECKLIST CHUáº¨N Bá»Š**

- [ ] Slide presentation (8-10 slides)
- [ ] Confusion matrix images (3 models)
- [ ] Model comparison chart
- [ ] Code demo
- [ ] Hiá»ƒu rÃµ CV mean, Precision, Recall, F1
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c amplification strategy
- [ ] Biáº¿t best hyperparameters vÃ  Ã½ nghÄ©a
- [ ] Chuáº©n bá»‹ tráº£ lá»i 12 cÃ¢u há»i trÃªn

---

### ğŸ¯ **ÄIá»‚M Máº NH Äá»‚ NHáº¤N Máº NH**

1. âœ… **Äáº¡t target 85-95%** vá»›i SVM 91.07%
2. âœ… **Amplification strategy sÃ¡ng táº¡o** Ä‘á»ƒ tÄƒng accuracy tá»« 62% â†’ 91%
3. âœ… **So sÃ¡nh Ä‘áº§y Ä‘á»§ 3 thuáº­t toÃ¡n** vÃ  giáº£i thÃ­ch rÃµ táº¡i sao chá»n SVM
4. âœ… **Recall cao (92%)** â†’ Ã­t bá» sÃ³t Fatigue â†’ quan trá»ng vá»›i á»©ng dá»¥ng thá»±c táº¿
5. âœ… **KhÃ´ng overfit** (CV mean â‰ˆ Test accuracy)
6. âœ… **CÃ³ demo thá»±c táº¿** vá»›i model Ä‘Ã£ train

---

**ChÃºc báº¡n bÃ¡o cÃ¡o giá»¯a ká»³ thÃ nh cÃ´ng! ğŸ‰**
