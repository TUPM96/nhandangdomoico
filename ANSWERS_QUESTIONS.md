# TRáº¢ Lá»œI CÃC CÃ‚U Há»I - Há»† THá»NG NHáº¬N Dáº NG Má»I CÆ 

---

## CÃ‚U 1: Sau khi cháº¡y ra code vÃ  cÃ³ káº¿t quáº£ 3 thuáº­t toÃ¡n, cáº§n lÃ m gÃ¬ tiáº¿p theo?

### âœ… CÃ¡c bÆ°á»›c cáº§n lÃ m sau khi cÃ³ káº¿t quáº£:

#### 1. **PhÃ¢n tÃ­ch vÃ  so sÃ¡nh káº¿t quáº£**
```bash
# Xem file so sÃ¡nh
cat models/model_comparison.csv
cat test_results/test_comparison.csv
```

**Káº¿t quáº£ thá»±c táº¿:**
| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| SVM   | 95.73%   | 96.73%    | 94.67% | 95.69%   |
| LDA   | 94.80%   | 95.90%    | 93.60% | 94.74%   |
| KNN   | 94.53%   | 95.63%    | 93.33% | 94.47%   |

**Káº¿t luáº­n:** SVM lÃ  model tá»‘t nháº¥t

#### 2. **ÄÃ¡nh giÃ¡ chi tiáº¿t model tá»‘t nháº¥t (SVM)**

**a) Confusion Matrix Analysis:**
```
True Negative (TN): 363  - Dá»± Ä‘oÃ¡n Ä‘Ãºng Non-Fatigue
False Positive (FP): 12  - Dá»± Ä‘oÃ¡n sai thÃ nh Fatigue
False Negative (FN): 20  - Dá»± Ä‘oÃ¡n sai thÃ nh Non-Fatigue
True Positive (TP): 355  - Dá»± Ä‘oÃ¡n Ä‘Ãºng Fatigue
```

**b) Best Hyperparameters:**
- C = 0.1
- kernel = 'rbf'
- gamma = 'scale'

#### 3. **Viáº¿t bÃ¡o cÃ¡o káº¿t quáº£**
Táº¡o file bÃ¡o cÃ¡o bao gá»“m:
- MÃ´ táº£ bÃ i toÃ¡n
- Dá»¯ liá»‡u (10 features, 2 classes)
- PhÆ°Æ¡ng phÃ¡p (LDA, KNN, SVM)
- Káº¿t quáº£ (accuracy, confusion matrix, etc.)
- Káº¿t luáº­n vÃ  khuyáº¿n nghá»‹

#### 4. **Deploy model tá»‘t nháº¥t**
```python
# Sá»­ dá»¥ng SVM model Ä‘á»ƒ predict
from train_models import FatigueMuscleClassifier

classifier = FatigueMuscleClassifier.load_model('models/svm_model.pkl')
# ... predict cho dá»¯ liá»‡u má»›i
```

#### 5. **Tá»‘i Æ°u hÃ³a thÃªm (náº¿u cáº§n)**
- Thu tháº­p thÃªm dá»¯ liá»‡u
- Feature engineering
- Thá»­ ensemble methods
- Hyperparameter tuning chi tiáº¿t hÆ¡n

---

## CÃ‚U 2: CV mean lÃ  bao nhiÃªu? CÃ¡ch tÃ­nh trong bÃ i

### ğŸ“Š Cross-Validation Mean (CV mean)

**CV mean** lÃ  **trung bÃ¬nh accuracy** cá»§a model trÃªn táº¥t cáº£ cÃ¡c folds trong Cross-Validation.

### Káº¿t quáº£ CV mean cá»§a 3 models:

| Model | CV Mean | CV Std | Min | Max |
|-------|---------|--------|-----|-----|
| **SVM** | **0.9524** | Â±0.0270 | 0.9356 | 0.9689 |
| LDA | 0.9524 | Â±0.0290 | 0.9356 | 0.9711 |
| KNN | 0.9484 | Â±0.0196 | 0.9356 | 0.9622 |

### ğŸ“ CÃ¡ch tÃ­nh CV mean:

#### CÃ´ng thá»©c:
```
CV_mean = (accuracy_fold1 + accuracy_fold2 + ... + accuracy_foldN) / N

CV_std = âˆš(Î£(accuracy_foldi - CV_mean)Â² / N)
```

#### VÃ­ dá»¥ vá»›i 5-fold CV:

**Giáº£ sá»­ SVM cÃ³ accuracy trÃªn 5 folds:**
- Fold 1: 0.9356
- Fold 2: 0.9467
- Fold 3: 0.9689
- Fold 4: 0.9511
- Fold 5: 0.9600

**TÃ­nh CV mean:**
```
CV_mean = (0.9356 + 0.9467 + 0.9689 + 0.9511 + 0.9600) / 5
        = 4.7623 / 5
        = 0.9524 (95.24%)
```

**TÃ­nh CV std:**
```
Variance = [(0.9356-0.9524)Â² + (0.9467-0.9524)Â² + (0.9689-0.9524)Â² +
            (0.9511-0.9524)Â² + (0.9600-0.9524)Â²] / 5
         = 0.000729

CV_std = âˆš0.000729 = 0.0270
```

### ğŸ’» Code trong bÃ i:

```python
from sklearn.model_selection import cross_val_score

# Thá»±c hiá»‡n 5-fold cross-validation
cv_scores = cross_val_score(model, X_train_scaled, y_train,
                            cv=5, scoring='accuracy')

# TÃ­nh CV mean vÃ  std
cv_mean = cv_scores.mean()  # 0.9524
cv_std = cv_scores.std()    # 0.0270

print(f"CV Mean: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")
# Output: CV Mean: 0.9524 (+/- 0.0540)
```

### ğŸ“ Ã nghÄ©a:

- **CV mean cao (>0.90)**: Model há»c tá»‘t, generalization tá»‘t
- **CV std tháº¥p (<0.05)**: Model stable, khÃ´ng overfitting
- **Min vÃ  Max gáº§n nhau**: Model consistent trÃªn cÃ¡c folds

**Káº¿t luáº­n:** CV mean = 0.9524 cho tháº¥y SVM cÃ³ kháº£ nÄƒng generalization ráº¥t tá»‘t!

---

## CÃ‚U 3: Váº½ sÆ¡ Ä‘á»“ khá»‘i thuáº­t toÃ¡n vÃ  lÆ°u Ä‘á»“ giáº£i thuáº­t cho há»‡ thá»‘ng

### ğŸ“Š SÆ  Äá»’ Tá»”NG QUAN Há»† THá»NG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Há»† THá»NG NHáº¬N Dáº NG Má»I CÆ                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 1: THU THáº¬P Dá»® LIá»†U (Data Collection)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: 10 features sinh lÃ½                                     â”‚
â”‚  - EMG signals (RMS, MAV, median_freq, mean_freq)               â”‚
â”‚  - Muscle metrics (force, tension)                              â”‚
â”‚  - Physiological (heart_rate)                                   â”‚
â”‚  - Activity (work_duration, rest_time, movement_frequency)      â”‚
â”‚  Output: Dataset vá»›i labels (0=Non-Fatigue, 1=Fatigue)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 2: TIá»€N Xá»¬ LÃ Dá»® LIá»†U (Data Preprocessing)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Chia train/test (75/25)                                     â”‚
â”‚  2. Chuáº©n hÃ³a dá»¯ liá»‡u (StandardScaler)                          â”‚
â”‚     - Mean = 0, Std = 1                                         â”‚
â”‚     - X_scaled = (X - Î¼) / Ïƒ                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 3: TRAINING MODELS (3 thuáº­t toÃ¡n)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚     LDA      â”‚    â”‚     KNN      â”‚    â”‚     SVM      â”‚      â”‚
â”‚  â”‚  (Linear)    â”‚    â”‚ (Instance)   â”‚    â”‚  (Kernel)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  GridSearchCV        GridSearchCV        GridSearchCV          â”‚
â”‚  - solver            - n_neighbors       - C                   â”‚
â”‚  - shrinkage         - weights           - kernel              â”‚
â”‚                      - metric            - gamma               â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚                    5-Fold Cross-Validation                      â”‚
â”‚                    TÃ¬m best parameters                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 4: EVALUATION (Test Models)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metrics:                                                        â”‚
â”‚  - Accuracy = (TP + TN) / Total                                 â”‚
â”‚  - Precision = TP / (TP + FP)                                   â”‚
â”‚  - Recall = TP / (TP + FN)                                      â”‚
â”‚  - F1-Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)   â”‚
â”‚  - Confusion Matrix                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 5: SO SÃNH VÃ€ CHá»ŒN MODEL Tá»T NHáº¤T                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  So sÃ¡nh accuracy, precision, recall, f1-score                  â”‚
â”‚  â†’ Chá»n SVM (Accuracy: 95.73%)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BÆ¯á»šC 6: DEPLOYMENT (Sá»­ dá»¥ng model)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Load model â†’ Predict cho dá»¯ liá»‡u má»›i                           â”‚
â”‚  Output: 0 (Non-Fatigue) hoáº·c 1 (Fatigue)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ LÆ¯U Äá»’ GIáº¢I THUáº¬T CHI TIáº¾T

#### A. LÆ¯U Äá»’ TRAINING:

```
        START
          â”‚
          â–¼
    [Load data]
          â”‚
          â–¼
    [Split train/test] â”€â”€â†’ 75% train, 25% test
          â”‚
          â–¼
    [Chuáº©n hÃ³a data]
    StandardScaler
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ For each model: â”‚
    â”‚ LDA, KNN, SVM   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    [Setup param grid]
          â”‚
          â–¼
    [GridSearchCV]
    â”œâ”€ 5-fold CV
    â”œâ”€ Try all param combinations
    â””â”€ Select best params
          â”‚
          â–¼
    [Train with best params]
          â”‚
          â–¼
    [Evaluate on test set]
    â”œâ”€ Accuracy
    â”œâ”€ Precision
    â”œâ”€ Recall
    â””â”€ F1-Score
          â”‚
          â–¼
    [Save model]
          â”‚
          â–¼
        END
```

#### B. LÆ¯U Äá»’ PREDICTION:

```
        START
          â”‚
          â–¼
    [Load trained model]
          â”‚
          â–¼
    [Input: 10 features]
    - emg_rms
    - emg_mav
    - emg_median_freq
    - emg_mean_freq
    - muscle_force
    - heart_rate
    - work_duration
    - rest_time
    - movement_frequency
    - muscle_tension
          â”‚
          â–¼
    [Chuáº©n hÃ³a input]
    Sá»­ dá»¥ng scaler Ä‘Ã£ fit
          â”‚
          â–¼
    [Model predict]
          â”‚
          â”œâ”€â”€â†’ [0] Non-Fatigue
          â”‚
          â””â”€â”€â†’ [1] Fatigue
          â”‚
          â–¼
    [Return prediction]
          â”‚
          â–¼
        END
```

#### C. LÆ¯U Äá»’ THUáº¬T TOÃN SVM:

```
        START
          â”‚
          â–¼
    [Input: Training data X, y]
          â”‚
          â–¼
    [Choose kernel function]
    â”œâ”€ Linear: K(x,x') = xÂ·x'
    â”œâ”€ RBF: K(x,x') = exp(-Î³||x-x'||Â²)
    â””â”€ Polynomial: K(x,x') = (Î³xÂ·x'+r)^d
          â”‚ (Chá»n RBF)
          â–¼
    [Map to higher dimension]
    Kernel trick
          â”‚
          â–¼
    [Find hyperplane]
    Maximize margin
    min 1/2||w||Â² + CÂ·Î£Î¾áµ¢
    subject to: yáµ¢(wÂ·xáµ¢+b) â‰¥ 1-Î¾áµ¢
          â”‚
          â–¼
    [Solve optimization]
    Quadratic programming
          â”‚
          â–¼
    [Identify support vectors]
    Points on margin boundary
          â”‚
          â–¼
    [Decision function]
    f(x) = sign(Î£ Î±áµ¢yáµ¢K(xáµ¢,x) + b)
          â”‚
          â–¼
    [Predict new data]
    â”œâ”€ f(x) > 0 â†’ Class 1 (Fatigue)
    â””â”€ f(x) < 0 â†’ Class 0 (Non-Fatigue)
          â”‚
          â–¼
        END
```

---

## CÃ‚U 4: CÃ¡ch tÃ­nh cÃ¡c há»‡ sá»‘ trong pháº§n test vÃ  pháº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh

### ğŸ“ CÃC Há»† Sá» QUAN TRá»ŒNG

#### A. TRONG PHáº¦N TRAINING:

##### 1. **Standardization (Chuáº©n hÃ³a) - StandardScaler**

**CÃ´ng thá»©c:**
```
X_scaled = (X - Î¼) / Ïƒ

Trong Ä‘Ã³:
- Î¼ (mu) = mean cá»§a feature
- Ïƒ (sigma) = standard deviation cá»§a feature
```

**VÃ­ dá»¥ vá»›i feature `emg_rms`:**
```python
# Training data
X_train['emg_rms'] = [0.15, 0.18, 0.20, 0.22, 0.25, ...]

# TÃ­nh mean vÃ  std
Î¼ = 0.21  # mean
Ïƒ = 0.05  # std

# Chuáº©n hÃ³a
X_scaled = (0.18 - 0.21) / 0.05 = -0.6
```

**âš ï¸ Quan trá»ng:** Pháº£i lÆ°u Î¼ vÃ  Ïƒ tá»« training set Ä‘á»ƒ dÃ¹ng cho test set!

##### 2. **LDA Coefficients (Há»‡ sá»‘ phÃ¢n biá»‡t tuyáº¿n tÃ­nh)**

**CÃ´ng thá»©c LDA:**
```
w = Sw^(-1) Ã— (Î¼â‚ - Î¼â‚€)

Trong Ä‘Ã³:
- w: vector há»‡ sá»‘ (discriminant coefficients)
- Sw: within-class scatter matrix
- Î¼â‚, Î¼â‚€: mean vectors cá»§a 2 classes
```

**Sw (Within-class scatter matrix):**
```
Sw = Î£(xáµ¢ - Î¼class)Ã—(xáµ¢ - Î¼class)áµ€
```

**Decision function:**
```
f(x) = wáµ€x + b

Náº¿u f(x) > 0: Predict class 1 (Fatigue)
Náº¿u f(x) < 0: Predict class 0 (Non-Fatigue)
```

**Code láº¥y coefficients:**
```python
# Sau khi train LDA
lda_model.coef_          # Shape: (1, 10) - 10 há»‡ sá»‘ cho 10 features
lda_model.intercept_     # Bias term

# VÃ­ dá»¥:
# coef_ = [0.45, 0.38, -0.62, -0.58, 0.28, 0.35, 0.42, -0.31, -0.27, 0.33]
```

##### 3. **KNN - KhÃ´ng cÃ³ há»‡ sá»‘ training!**

KNN lÃ  **instance-based learning** - khÃ´ng cÃ³ há»‡ sá»‘.

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- LÆ°u toÃ n bá»™ training data
- Khi predict: TÃ­nh khoáº£ng cÃ¡ch Ä‘áº¿n k neighbors gáº§n nháº¥t
- Vote theo class cá»§a k neighbors

**Distance metrics:**
```
Euclidean: d(x,y) = âˆš(Î£(xáµ¢-yáµ¢)Â²)
Manhattan: d(x,y) = Î£|xáµ¢-yáµ¢|
```

##### 4. **SVM Coefficients (Support Vectors vÃ  Î±)**

**CÃ´ng thá»©c SVM:**
```
f(x) = Î£ Î±áµ¢yáµ¢K(xáµ¢,x) + b

Trong Ä‘Ã³:
- Î±áµ¢: Lagrange multipliers (há»‡ sá»‘)
- yáµ¢: labels (-1 hoáº·c +1)
- K: kernel function
- xáµ¢: support vectors
- b: bias
```

**RBF Kernel:**
```
K(x,x') = exp(-Î³||x-x'||Â²)

Î³ = 1/(2ÏƒÂ²)  # gamma parameter
```

**Code láº¥y SVM coefficients:**
```python
# Sau khi train SVM
svm_model.support_vectors_   # Support vectors
svm_model.dual_coef_         # Î± Ã— y
svm_model.intercept_         # Bias b

# Vá»›i RBF kernel:
# dual_coef_: (1, n_support_vectors)
# support_vectors_: (n_support_vectors, 10)
```

#### B. TRONG PHáº¦N TESTING:

##### 1. **Prediction Process**

**BÆ°á»›c 1: Chuáº©n hÃ³a test data**
```python
# Sá»­ dá»¥ng Î¼ vÃ  Ïƒ tá»« training set
X_test_scaled = (X_test - Î¼_train) / Ïƒ_train
```

**BÆ°á»›c 2: Apply decision function**

**LDA:**
```python
score = w^T Ã— X_test_scaled + b
prediction = 1 if score > 0 else 0
```

**KNN:**
```python
# TÃ¬m k=15 neighbors gáº§n nháº¥t
distances = [euclidean(X_test, X_train[i]) for all i]
k_nearest = sorted(distances)[:15]
prediction = majority_vote(k_nearest_labels)
```

**SVM:**
```python
# RBF kernel
score = Î£ Î±áµ¢yáµ¢ Ã— exp(-Î³||X_test - xáµ¢||Â²) + b
prediction = 1 if score > 0 else 0
```

##### 2. **Metrics Calculation**

**Confusion Matrix:**
```
                 Predicted
               Non-F  Fatigue
Actual Non-F  â”‚ TN  â”‚  FP  â”‚
       Fatigueâ”‚ FN  â”‚  TP  â”‚

VÃ­ dá»¥ SVM:
               Non-F  Fatigue
       Non-F  â”‚ 363 â”‚  12  â”‚
       Fatigueâ”‚  20 â”‚ 355  â”‚
```

**Accuracy:**
```
Accuracy = (TP + TN) / Total
         = (355 + 363) / 750
         = 718 / 750
         = 0.9573 (95.73%)
```

**Precision:**
```
Precision = TP / (TP + FP)
          = 355 / (355 + 12)
          = 355 / 367
          = 0.9673 (96.73%)
```

**Recall (Sensitivity):**
```
Recall = TP / (TP + FN)
       = 355 / (355 + 20)
       = 355 / 375
       = 0.9467 (94.67%)
```

**F1-Score:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
   = 2 Ã— (0.9673 Ã— 0.9467) / (0.9673 + 0.9467)
   = 2 Ã— 0.9153 / 1.9140
   = 0.9569 (95.69%)
```

### ğŸ’» Code tÃ­nh toÃ¡n trong bÃ i:

```python
# 1. Training - Láº¥y coefficients
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# Chuáº©n hÃ³a
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# LÆ°u mean vÃ  std
mu = scaler.mean_         # [0.21, 0.175, 73.5, ...]
sigma = scaler.scale_     # [0.05, 0.045, 12, ...]

# Train SVM
svm = SVC(C=0.1, kernel='rbf', gamma='scale')
svm.fit(X_train_scaled, y_train)

# Láº¥y há»‡ sá»‘
support_vectors = svm.support_vectors_
dual_coef = svm.dual_coef_
intercept = svm.intercept_

print(f"Sá»‘ support vectors: {len(support_vectors)}")
print(f"Intercept (b): {intercept}")

# 2. Testing - Sá»­ dá»¥ng há»‡ sá»‘
X_test_scaled = scaler.transform(X_test)  # DÃ¹ng mu, sigma tá»« training
y_pred = svm.predict(X_test_scaled)

# 3. TÃ­nh metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
```

---

## CÃ‚U 5: CÃ¡ch xem cÃ¡c biá»ƒu Ä‘á»“ á»Ÿ SVM

### ğŸ“Š CÃC LOáº I BIá»‚U Äá»’ TRONG SVM

Há»‡ thá»‘ng Ä‘Ã£ tá»± Ä‘á»™ng táº¡o cÃ¡c biá»ƒu Ä‘á»“ khi cháº¡y. Xem táº¡i:

```bash
# Confusion matrices
ls plots/

# Biá»ƒu Ä‘á»“ so sÃ¡nh
ls test_results/
```

#### 1. **CONFUSION MATRIX** (Quan trá»ng nháº¥t!)

**File:** `plots/svm_confusion_matrix.png`

```
Confusion Matrix - SVM
                Predicted
           Non-Fatigue  Fatigue
Actual     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Non-F      â”‚   363   â”‚   12    â”‚  â† 12 False Positives
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Fatigue    â”‚   20    â”‚   355   â”‚  â† 20 False Negatives
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†‘
        20 FN: NghiÃªm trá»ng!
        (Dá»± Ä‘oÃ¡n Non-Fatigue nhÆ°ng thá»±c táº¿ Fatigue)
```

**CÃ¡ch Ä‘á»c:**
- **ÄÆ°á»ng chÃ©o (363, 355)**: Predictions Ä‘Ãºng âœ“
- **NgoÃ i Ä‘Æ°á»ng chÃ©o (12, 20)**: Predictions sai âœ—
- **FP = 12**: 12 ngÆ°á»i khÃ´ng má»i bá»‹ dá»± Ä‘oÃ¡n nháº§m lÃ  má»i
- **FN = 20**: 20 ngÆ°á»i má»i bá»‹ dá»± Ä‘oÃ¡n nháº§m lÃ  khÃ´ng má»i âš ï¸

#### 2. **BIá»‚U Äá»’ SO SÃNH 3 MODELS**

**File:** `test_results/models_comparison.png`

Biá»ƒu Ä‘á»“ bar chart so sÃ¡nh 4 metrics cá»§a 3 models:
- Accuracy
- Precision
- Recall
- F1-Score

**NhÃ¬n vÃ o biá»ƒu Ä‘á»“:**
- SVM cÃ³ cá»™t cao nháº¥t á»Ÿ táº¥t cáº£ metrics
- ÄÆ°á»ng target 85% (Ä‘Æ°á»ng Ä‘á») á»Ÿ biá»ƒu Ä‘á»“ Accuracy
- Táº¥t cáº£ models Ä‘á»u vÆ°á»£t target

#### 3. **Táº O THÃŠM CÃC BIá»‚U Äá»’ NÃ‚NG CAO**

##### A. Decision Boundary (2D projection)

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA

# Load model vÃ  data
from train_models import FatigueMuscleClassifier
import pandas as pd

classifier = FatigueMuscleClassifier.load_model('models/svm_model.pkl')
df = pd.read_csv('data_generated/test_data.csv')

# Láº¥y features vÃ  labels
feature_cols = [col for col in df.columns if col not in ['label', 'class_name']]
X = df[feature_cols].values
y = df['label'].values

# Giáº£m xuá»‘ng 2D báº±ng PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(classifier.scaler.transform(X))

# Plot decision boundary
plt.figure(figsize=(10, 8))

# Create mesh
h = 0.02
x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# Predict trÃªn mesh
# (LÆ°u Ã½: cáº§n transform ngÆ°á»£c PCA, code phá»©c táº¡p hÆ¡n)

# Plot points
scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='coolwarm',
                     edgecolors='black', s=50, alpha=0.8)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('SVM Decision Boundary (2D PCA Projection)')
plt.colorbar(scatter, label='Class')
plt.savefig('plots/svm_decision_boundary.png', dpi=300)
plt.show()
```

##### B. Feature Importance (cho SVM vá»›i linear kernel)

```python
# Train SVM vá»›i linear kernel Ä‘á»ƒ xem feature importance
from sklearn.svm import SVC
import matplotlib.pyplot as plt

svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train_scaled, y_train)

# Láº¥y coefficients
importance = np.abs(svm_linear.coef_[0])

# Plot
features = ['emg_rms', 'emg_mav', 'emg_median_freq', 'emg_mean_freq',
            'muscle_force', 'heart_rate', 'work_duration', 'rest_time',
            'movement_frequency', 'muscle_tension']

plt.figure(figsize=(10, 6))
plt.barh(features, importance)
plt.xlabel('Feature Importance (Absolute Coefficient)')
plt.title('SVM Linear Kernel - Feature Importance')
plt.tight_layout()
plt.savefig('plots/svm_feature_importance.png', dpi=300)
plt.show()
```

##### C. Learning Curve

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

# TÃ­nh learning curve
train_sizes, train_scores, test_scores = learning_curve(
    classifier.model, X_scaled, y, cv=5, n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# TÃ­nh mean vÃ  std
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training score', color='blue')
plt.fill_between(train_sizes, train_mean - train_std,
                 train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, test_mean, label='Cross-validation score', color='red')
plt.fill_between(train_sizes, test_mean - test_std,
                 test_mean + test_std, alpha=0.1, color='red')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy Score')
plt.title('SVM Learning Curve')
plt.legend(loc='best')
plt.grid(alpha=0.3)
plt.savefig('plots/svm_learning_curve.png', dpi=300)
plt.show()
```

##### D. ROC Curve (náº¿u SVM cÃ³ probability=True)

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Láº¥y probability predictions
y_proba = classifier.model.predict_proba(X_test_scaled)[:, 1]

# TÃ­nh ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVM - ROC Curve')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.savefig('plots/svm_roc_curve.png', dpi=300)
plt.show()
```

### ğŸ“ CÃ¡c file biá»ƒu Ä‘á»“ hiá»‡n cÃ³:

```bash
plots/
â”œâ”€â”€ lda_confusion_matrix.png     # LDA confusion matrix
â”œâ”€â”€ knn_confusion_matrix.png     # KNN confusion matrix
â””â”€â”€ svm_confusion_matrix.png     # SVM confusion matrix â­

test_results/
â””â”€â”€ models_comparison.png        # So sÃ¡nh 3 models â­
```

### ğŸ” CÃ¡ch phÃ¢n tÃ­ch biá»ƒu Ä‘á»“ SVM:

1. **Confusion Matrix**:
   - ÄÆ°á»ng chÃ©o cao = tá»‘t
   - FN (False Negative) quan trá»ng hÆ¡n FP trong bÃ i toÃ¡n nÃ y

2. **Comparison Chart**:
   - SVM pháº£i cÃ³ cá»™t cao nháº¥t
   - Táº¥t cáº£ metrics > 85%

3. **Decision Boundary** (náº¿u táº¡o):
   - Xem SVM táº¡o boundary nhÆ° tháº¿ nÃ o
   - Support vectors náº±m gáº§n boundary

4. **Learning Curve** (náº¿u táº¡o):
   - Training score vÃ  CV score gáº§n nhau = khÃ´ng overfit
   - Cáº£ 2 Ä‘á»u cao = model tá»‘t

---

## CÃ‚U 6: BÃ¡o cÃ¡o giá»¯a ká»³ - Cáº§n chuáº©n bá»‹ gÃ¬? CÃ¢u há»i nÃ o sáº½ Ä‘Æ°á»£c há»i?

### ğŸ“‹ Ná»˜I DUNG BÃO CÃO GIá»®A Ká»²

#### A. Cáº¤U TRÃšC BÃO CÃO (Slides PowerPoint/PDF)

##### **1. SLIDE GIá»šI THIá»†U (1-2 slides)**
- TÃªn Ä‘á» tÃ i: "Há»‡ Thá»‘ng Nháº­n Dáº¡ng Má»i CÆ¡ sá»­ dá»¥ng Machine Learning"
- Há» tÃªn, MSSV
- Giáº£ng viÃªn hÆ°á»›ng dáº«n
- NgÃ y bÃ¡o cÃ¡o

##### **2. Má»¤C TIÃŠU & BÃ€I TOÃN (2-3 slides)**

**Ná»™i dung:**
- BÃ i toÃ¡n: PhÃ¢n loáº¡i tráº¡ng thÃ¡i má»i cÆ¡ (Fatigue/Non-Fatigue)
- Má»¥c tiÃªu: XÃ¢y dá»±ng model ML vá»›i accuracy 85-95%
- á»¨ng dá»¥ng thá»±c táº¿:
  - GiÃ¡m sÃ¡t sá»©c khá»e váº­n Ä‘á»™ng viÃªn
  - PhÃ²ng trÃ¡nh cháº¥n thÆ°Æ¡ng
  - Tá»‘i Æ°u hÃ³a lá»‹ch táº­p luyá»‡n

**Slide máº«u:**
```
BÃ€I TOÃN

Input: 10 features sinh lÃ½
â”œâ”€ EMG signals (RMS, MAV, freq)
â”œâ”€ Muscle metrics (force, tension)
â”œâ”€ Physiological (heart rate)
â””â”€ Activity (duration, rest, movement)

Output: 2 classes
â”œâ”€ 0: Non-Fatigue (KhÃ´ng má»i)
â””â”€ 1: Fatigue (Má»i)

Má»¥c tiÃªu: Accuracy â‰¥ 85%
```

##### **3. Dá»® LIá»†U (2-3 slides)**

**Slide 1: MÃ´ táº£ dá»¯ liá»‡u**
```
Dá»® LIá»†U

Tá»•ng sá»‘ máº«u: 3000
â”œâ”€ Training: 2250 (75%)
â””â”€ Testing: 750 (25%)

PhÃ¢n bá»‘ classes:
â”œâ”€ Non-Fatigue: 1500 máº«u (50%)
â””â”€ Fatigue: 1500 máº«u (50%)
â†’ Balanced dataset âœ“
```

**Slide 2: 10 Features**
```
CÃC FEATURES

1. EMG Signals (Äiá»‡n cÆ¡)
   - emg_rms: 0.05-0.50 mV
   - emg_mav: 0.04-0.40 mV
   - emg_median_freq: 40-120 Hz
   - emg_mean_freq: 45-125 Hz

2. Muscle Metrics
   - muscle_force: 10-80 N
   - muscle_tension: 10-90

3. Physiological
   - heart_rate: 50-140 bpm

4. Activity
   - work_duration: 1-90 phÃºt
   - rest_time: 0.5-20 phÃºt
   - movement_frequency: 5-40 láº§n/phÃºt
```

**Slide 3: PhÃ¢n bá»‘ dá»¯ liá»‡u (báº£ng thá»‘ng kÃª)**
```
THá»NG KÃŠ Dá»® LIá»†U

                Non-Fatigue    Fatigue
emg_rms            0.18         0.24
emg_mav            0.15         0.20
median_freq        78           68
heart_rate         80           90
muscle_force       42           36
muscle_tension     40           58
...

â†’ CÃ³ sá»± khÃ¡c biá»‡t rÃµ rÃ ng giá»¯a 2 classes
```

##### **4. PHÆ¯Æ NG PHÃP (4-5 slides)**

**Slide 1: Tá»•ng quan 3 thuáº­t toÃ¡n**
```
3 THUáº¬T TOÃN MACHINE LEARNING

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LDA    â”‚   KNN    â”‚     SVM     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear  â”‚Instance  â”‚   Kernel    â”‚
â”‚ Fast    â”‚ Simple   â”‚  Powerful   â”‚
â”‚ Stable  â”‚ Flexible â”‚  Accurate   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Slide 2: LDA**
```
LINEAR DISCRIMINANT ANALYSIS

NguyÃªn lÃ½:
- TÃ¬m Ä‘Æ°á»ng tháº³ng (hyperplane) phÃ¢n tÃ¡ch 2 classes
- Maximize between-class variance
- Minimize within-class variance

CÃ´ng thá»©c:
w = Sw^(-1) Ã— (Î¼â‚ - Î¼â‚€)

Best parameters:
- solver: lsqr
- shrinkage: auto
```

**Slide 3: KNN**
```
K-NEAREST NEIGHBORS

NguyÃªn lÃ½:
- Instance-based learning
- Classify dá»±a trÃªn k neighbors gáº§n nháº¥t
- Voting theo majority class

Best parameters:
- n_neighbors: 15
- weights: distance
- metric: manhattan

Distance formula:
d(x,y) = Î£|xáµ¢-yáµ¢|
```

**Slide 4: SVM**
```
SUPPORT VECTOR MACHINE

NguyÃªn lÃ½:
- TÃ¬m hyperplane vá»›i margin lá»›n nháº¥t
- Sá»­ dá»¥ng kernel trick cho non-linear
- Support vectors: Ä‘iá»ƒm trÃªn margin

Best parameters:
- C: 0.1
- kernel: RBF
- gamma: scale

Kernel RBF:
K(x,x') = exp(-Î³||x-x'||Â²)
```

**Slide 5: Quy trÃ¬nh**
```
QUY TRÃŒNH Xá»¬ LÃ

Data â†’ Preprocess â†’ Train â†’ Test â†’ Evaluate
  â”‚         â”‚          â”‚       â”‚        â”‚
  â”‚         â”‚          â”‚       â”‚        â””â”€â†’ Metrics
  â”‚         â”‚          â”‚       â””â”€â†’ Test set (750)
  â”‚         â”‚          â””â”€â†’ GridSearchCV + 5-fold CV
  â”‚         â””â”€â†’ StandardScaler (mean=0, std=1)
  â””â”€â†’ 3000 samples, 10 features
```

##### **5. Káº¾T QUáº¢ (4-5 slides)** â­ QUAN TRá»ŒNG NHáº¤T

**Slide 1: Báº£ng so sÃ¡nh tá»•ng quan**
```
Káº¾T QUáº¢ SO SÃNH 3 MODELS

â•”â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Model   â•‘ Accuracy â•‘ Precision â•‘ Recall â•‘ F1-Score â•‘
â• â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•£
â•‘ LDA     â•‘  94.80%  â•‘   95.90%  â•‘ 93.60% â•‘  94.74%  â•‘
â•‘ KNN     â•‘  94.53%  â•‘   95.63%  â•‘ 93.33% â•‘  94.47%  â•‘
â•‘ SVM     â•‘  95.73%  â•‘   96.73%  â•‘ 94.67% â•‘  95.69%  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•

âœ“ Táº¤T Cáº¢ Äáº T Má»¤C TIÃŠU â‰¥ 85%
âœ“ SVM Tá»T NHáº¤T: 95.73%
```

**Slide 2: Confusion Matrix SVM**
```
CONFUSION MATRIX - SVM

                Predicted
           Non-F  Fatigue
Actual     â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
Non-F      â”‚ 363  â”‚  12  â”‚ Precision = 96.0%
           â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
Fatigue    â”‚  20  â”‚ 355  â”‚ Recall = 94.7%
           â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Accuracy = (363+355)/750 = 95.73%

â†’ Chá»‰ 32 errors / 750 samples
```

**Slide 3: Cross-Validation Results**
```
CROSS-VALIDATION (5-fold)

         CV Mean    CV Std    Min      Max
SVM      0.9524    Â±0.0270   0.9356   0.9689
LDA      0.9524    Â±0.0290   0.9356   0.9711
KNN      0.9484    Â±0.0196   0.9356   0.9622

â†’ Stable models, khÃ´ng overfit
â†’ CV Mean cao: generalization tá»‘t
```

**Slide 4: Biá»ƒu Ä‘á»“ so sÃ¡nh (chÃ¨n áº£nh)**
- ChÃ¨n file `test_results/models_comparison.png`
- Giáº£i thÃ­ch: SVM cÃ³ cá»™t cao nháº¥t á»Ÿ táº¥t cáº£ metrics

**Slide 5: PhÃ¢n tÃ­ch SVM**
```
Táº I SAO SVM Tá»T NHáº¤T?

âœ“ Accuracy cao nháº¥t: 95.73%
âœ“ Precision cao: 96.73% (Ã­t FP)
âœ“ Recall tá»‘t: 94.67% (Ã­t FN)
âœ“ RBF kernel xá»­ lÃ½ non-linear tá»‘t
âœ“ CV score stable (std tháº¥p)
âœ“ Best params tá»« GridSearchCV

Best parameters:
- C = 0.1: regularization vá»«a pháº£i
- kernel = RBF: non-linear decision boundary
- gamma = scale: tá»± Ä‘á»™ng tÃ­nh optimal
```

##### **6. DEMO (1-2 slides)**

**Slide: Demo prediction**
```
DEMO Há»† THá»NG

Input (vÃ­ dá»¥ ngÆ°á»i má»i):
- emg_rms: 0.28 mV â†‘
- heart_rate: 95 bpm â†‘
- work_duration: 45 phÃºt â†‘
- rest_time: 3 phÃºt â†“
- muscle_tension: 70 â†‘

â†’ SVM Predict: FATIGUE (100% confidence)

á»¨ng dá»¥ng:
- Real-time monitoring
- Alert system
- Training optimization
```

##### **7. Káº¾T LUáº¬N (1-2 slides)**

```
Káº¾T LUáº¬N

âœ“ ÄÃ£ xÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng nháº­n dáº¡ng má»i cÆ¡
âœ“ Sá»­ dá»¥ng 3 thuáº­t toÃ¡n: LDA, KNN, SVM
âœ“ Äáº¡t má»¥c tiÃªu: Accuracy 85-95%
âœ“ SVM lÃ  model tá»‘t nháº¥t: 95.73%

Æ¯u Ä‘iá»ƒm:
- Accuracy cao, stable
- Xá»­ lÃ½ Ä‘Æ°á»£c non-linear relationships
- GridSearchCV tÃ¬m optimal params

Háº¡n cháº¿ & HÆ°á»›ng phÃ¡t triá»ƒn:
- Data synthetic (cáº§n real-world data)
- ThÃªm features (lactate, oxygen, etc.)
- Deploy real-time system
- Thá»­ ensemble methods
```

---

#### B. CÃ‚U Há»I THÆ¯á»œNG Gáº¶P KHI BÃO CÃO

##### ğŸ”¥ **NHÃ“M 1: CÃ‚U Há»I Vá»€ Dá»® LIá»†U**

**Q1: "Dá»¯ liá»‡u láº¥y tá»« Ä‘Ã¢u? CÃ³ pháº£i dá»¯ liá»‡u tháº­t khÃ´ng?"**
```
Tráº£ lá»i:
- Dá»¯ liá»‡u lÃ  synthetic data Ä‘Æ°á»£c generate dá»±a trÃªn nghiÃªn cá»©u EMG
- PhÃ¢n bá»‘ features dá»±a trÃªn cÃ¡c paper vá» muscle fatigue
- Táº¡o overlap giá»¯a 2 classes Ä‘á»ƒ realistic (khÃ´ng 100% separable)
- 3000 samples, balanced classes (50-50)

Káº¿ hoáº¡ch:
- Sáº½ thu tháº­p real-world data tá»« lab
- Sá»­ dá»¥ng EMG sensors, heart rate monitors
```

**Q2: "Táº¡i sao chá»n 10 features nÃ y?"**
```
Tráº£ lá»i:
- Dá»±a trÃªn research vá» muscle fatigue detection
- EMG signals: indicator chÃ­nh cá»§a fatigue
- Physiological: heart rate tÄƒng khi má»i
- Activity metrics: work/rest ratio quan trá»ng

References:
- [Paper vá» EMG vÃ  fatigue]
- [WHO guidelines on muscle fatigue]
```

**Q3: "Táº¡i sao chia 75/25 train/test?"**
```
Tráº£ lá»i:
- ÄÃ¢y lÃ  tá»· lá»‡ standard trong ML
- 75% Ä‘á»§ data cho training (2250 samples)
- 25% Ä‘á»§ lá»›n Ä‘á»ƒ evaluate reliably (750 samples)
- CÃ³ thá»ƒ dÃ¹ng 80/20 hoáº·c 70/30 tÃ¹y dataset size
```

##### ğŸ”¥ **NHÃ“M 2: CÃ‚U Há»I Vá»€ THUáº¬T TOÃN**

**Q4: "Táº¡i sao chá»n 3 thuáº­t toÃ¡n nÃ y?"**
```
Tráº£ lá»i:
- LDA: Linear baseline, fast, interpretable
- KNN: Simple, non-parametric, good for comparison
- SVM: State-of-the-art, powerful vá»›i kernel trick

Coverage:
- Linear (LDA) vs Non-linear (SVM-RBF)
- Parametric (LDA, SVM) vs Non-parametric (KNN)
- Discriminative models (all 3)
```

**Q5: "Giáº£i thÃ­ch cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a SVM?"**
```
Tráº£ lá»i:
1. TÃ¬m hyperplane phÃ¢n tÃ¡ch 2 classes
2. Maximize margin (khoáº£ng cÃ¡ch tá»« hyperplane Ä‘áº¿n Ä‘iá»ƒm gáº§n nháº¥t)
3. Support vectors: Ä‘iá»ƒm náº±m trÃªn margin
4. RBF kernel: map data lÃªn khÃ´ng gian cao hÆ¡n
5. Decision function: f(x) = Î£ Î±áµ¢yáµ¢K(xáµ¢,x) + b

Æ¯u Ä‘iá»ƒm:
- Xá»­ lÃ½ non-linear tá»‘t vá»›i kernel
- Robust vá»›i outliers
- Generalization tá»‘t
```

**Q6: "GridSearchCV lÃ  gÃ¬? Táº¡i sao dÃ¹ng?"**
```
Tráº£ lá»i:
- Tá»± Ä‘á»™ng tÃ¬m best hyperparameters
- Thá»­ táº¥t cáº£ combinations trong param grid
- Evaluate báº±ng cross-validation

VÃ­ dá»¥ SVM:
- Grid: C=[0.1,1,10,100], kernel=[rbf,linear], gamma=[...]
- Total combinations: 72
- Vá»›i 5-fold CV: 72Ã—5 = 360 fits
- Chá»n combo cÃ³ CV score cao nháº¥t

â†’ Best: C=0.1, kernel=rbf, gamma=scale
```

**Q7: "Cross-validation lÃ  gÃ¬? Táº¡i sao dÃ¹ng 5-fold?"**
```
Tráº£ lá»i:
- Chia training data thÃ nh 5 folds
- Má»—i láº§n: 4 folds train, 1 fold validate
- Láº·p 5 láº§n, má»—i fold lÃ m validation 1 láº§n
- CV mean = average cá»§a 5 scores

Táº¡i sao 5-fold?
- Standard choice (balance giá»¯a bias-variance)
- 3-fold: quÃ¡ Ã­t, high variance
- 10-fold: computational expensive
- 5-fold: optimal trade-off
```

##### ğŸ”¥ **NHÃ“M 3: CÃ‚U Há»I Vá»€ Káº¾T QUáº¢**

**Q8: "Accuracy 95.73% cÃ³ tá»‘t khÃ´ng? So vá»›i cÃ¡c nghiÃªn cá»©u khÃ¡c?"**
```
Tráº£ lá»i:
- 95.73% lÃ  ráº¥t tá»‘t cho bÃ i toÃ¡n classification
- VÆ°á»£t target (85-95%) âœ“
- So vá»›i research papers: comparable
  - [Paper 1]: 92-94% vá»›i EMG
  - [Paper 2]: 88-93% vá»›i multi-modal sensors

ÄÃ¡nh giÃ¡:
- Training set: 2250 samples
- Test set: 750 samples (Ä‘á»™c láº­p)
- CV mean: 0.9524 (stable)
```

**Q9: "Táº¡i sao SVM tá»‘t hÆ¡n LDA vÃ  KNN?"**
```
Tráº£ lá»i:
              SVM    LDA    KNN
Accuracy      95.73  94.80  94.53
CV Mean       0.9524 0.9524 0.9484
Stability     High   High   Medium

LÃ½ do SVM tá»‘t hÆ¡n:
1. RBF kernel xá»­ lÃ½ non-linear relationships
2. Margin maximization â†’ generalization tá»‘t
3. Robust vá»›i noise trong data
4. GridSearchCV tÃ¬m Ä‘Æ°á»£c optimal params

LDA vs KNN:
- LDA: fast, linear assumption
- KNN: simple, nhÆ°ng sensitive vá»›i noise
```

**Q10: "False Negative vs False Positive - cÃ¡i nÃ o quan trá»ng hÆ¡n?"**
```
Tráº£ lá»i:
Trong bÃ i toÃ¡n nÃ y:

False Negative (20): NghiÃªm trá»ng hÆ¡n! âš ï¸
- Dá»± Ä‘oÃ¡n Non-Fatigue nhÆ°ng thá»±c táº¿ Fatigue
- NgÆ°á»i Ä‘ang má»i nhÆ°ng há»‡ thá»‘ng khÃ´ng phÃ¡t hiá»‡n
- â†’ Tiáº¿p tá»¥c táº­p luyá»‡n â†’ nguy cÆ¡ cháº¥n thÆ°Æ¡ng

False Positive (12): Ãt nghiÃªm trá»ng hÆ¡n
- Dá»± Ä‘oÃ¡n Fatigue nhÆ°ng thá»±c táº¿ Non-Fatigue
- â†’ Nghá»‰ thÃªm, an toÃ n hÆ¡n

â†’ NÃªn optimize Ä‘á»ƒ giáº£m FN (tÄƒng Recall)
â†’ CÃ³ thá»ƒ cháº¥p nháº­n FP cao hÆ¡n má»™t chÃºt
```

**Q11: "Precision 96.73% nghÄ©a lÃ  gÃ¬?"**
```
Tráº£ lá»i:
Precision = TP/(TP+FP) = 355/(355+12) = 96.73%

NghÄ©a:
- Trong 367 láº§n dá»± Ä‘oÃ¡n Fatigue
- CÃ³ 355 láº§n Ä‘Ãºng (96.73%)
- Chá»‰ 12 láº§n sai (3.27%)

â†’ Khi há»‡ thá»‘ng nÃ³i "Fatigue", tin tÆ°á»Ÿng Ä‘Æ°á»£c 96.73%
```

**Q12: "Recall 94.67% nghÄ©a lÃ  gÃ¬?"**
```
Tráº£ lá»i:
Recall = TP/(TP+FN) = 355/(355+20) = 94.67%

NghÄ©a:
- CÃ³ 375 ngÆ°á»i thá»±c táº¿ Fatigue
- PhÃ¡t hiá»‡n Ä‘Ãºng 355 ngÆ°á»i (94.67%)
- Bá» sÃ³t 20 ngÆ°á»i (5.33%)

â†’ PhÃ¡t hiá»‡n Ä‘Æ°á»£c 94.67% trÆ°á»ng há»£p má»i thá»±c táº¿
```

##### ğŸ”¥ **NHÃ“M 4: CÃ‚U Há»I Ká»¸ THUáº¬T**

**Q13: "StandardScaler lÃ m gÃ¬? Táº¡i sao cáº§n?"**
```
Tráº£ lá»i:
StandardScaler: Chuáº©n hÃ³a features vá» mean=0, std=1

CÃ´ng thá»©c:
X_scaled = (X - Î¼) / Ïƒ

Táº¡i sao cáº§n?
1. Features cÃ³ scale khÃ¡c nhau:
   - emg_rms: 0.05-0.50
   - heart_rate: 50-140
   - muscle_tension: 10-90

2. KhÃ´ng chuáº©n hÃ³a â†’ features lá»›n dominate
3. SVM vÃ  KNN sensitive vá»›i scale
4. LDA Ã­t sensitive nhÆ°ng váº«n nÃªn chuáº©n hÃ³a

âš ï¸ Quan trá»ng: DÃ¹ng Î¼ vÃ  Ïƒ tá»« training set cho test set!
```

**Q14: "Táº¡i sao SVM chá»n C=0.1? KhÃ´ng pháº£i cÃ ng lá»›n cÃ ng tá»‘t?"**
```
Tráº£ lá»i:
C lÃ  regularization parameter:

- C nhá» (0.1):
  - Margin rá»™ng hÆ¡n
  - Cháº¥p nháº­n nhiá»u violations
  - Generalization tá»‘t hÆ¡n
  - TrÃ¡nh overfit âœ“

- C lá»›n (100):
  - Margin háº¹p
  - Ãt violations
  - CÃ³ thá»ƒ overfit
  - Training accuracy cao nhÆ°ng test tháº¥p

GridSearchCV thá»­ [0.1, 1, 10, 100]
â†’ C=0.1 cho CV score cao nháº¥t
â†’ Balance giá»¯a training fit vÃ  generalization
```

**Q15: "RBF kernel hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?"**
```
Tráº£ lá»i:
RBF (Radial Basis Function) kernel:

K(x, x') = exp(-Î³ ||x - x'||Â²)

Trong Ä‘Ã³:
- Î³ (gamma): controls influence radius
- ||x - x'||: Euclidean distance

CÃ¡ch hoáº¡t Ä‘á»™ng:
1. Map data lÃªn khÃ´ng gian vÃ´ háº¡n chiá»u
2. KhÃ´ng cáº§n compute explicit mapping
3. Kernel trick: chá»‰ cáº§n tÃ­nh K(x, x')

Î³ = 'scale':
Î³ = 1 / (n_features Ã— variance)
  = 1 / (10 Ã— var(X))

Æ¯u Ä‘iá»ƒm:
- Xá»­ lÃ½ non-linear relationships
- Smooth decision boundary
- Works well khi classes cÃ³ shape phá»©c táº¡p
```

##### ğŸ”¥ **NHÃ“M 5: CÃ‚U Há»I Vá»€ á»¨NG Dá»¤NG**

**Q16: "Há»‡ thá»‘ng nÃ y á»©ng dá»¥ng nhÆ° tháº¿ nÃ o trong thá»±c táº¿?"**
```
Tráº£ lá»i:

1. Sports Science:
   - Monitor váº­n Ä‘á»™ng viÃªn trong training
   - Alert khi detect fatigue
   - Optimize training schedule

2. Occupational Health:
   - GiÃ¡m sÃ¡t cÃ´ng nhÃ¢n nhÃ  mÃ¡y
   - PhÃ²ng trÃ¡nh tai náº¡n do má»i
   - Improve productivity vÃ  safety

3. Rehabilitation:
   - Monitor bá»‡nh nhÃ¢n phá»¥c há»“i chá»©c nÄƒng
   - Äáº£m báº£o khÃ´ng overwork
   - Track progress

4. Military:
   - Monitor soldiers trong mission
   - Prevent fatigue-related errors
   - Optimize performance

Flow:
Sensors â†’ Data collection â†’ Preprocessing â†’ Model â†’
Alert system â†’ Coach/Doctor decision
```

**Q17: "LÃ m sao deploy há»‡ thá»‘ng nÃ y?"**
```
Tráº£ lá»i:

Architecture:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EMG Sensors â”‚â”€â”
â”‚ HR Monitor  â”‚â”€â”¼â†’ [Data Collection]
â”‚ Accelero... â”‚â”€â”˜        â†“
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   [Preprocessing]
                         â†“
                  [Load SVM Model]
                         â†“
                  [Predict Fatigue]
                         â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                 â”‚
            Fatigue         Non-Fatigue
               â”‚                 â”‚
          [Send Alert]      [Continue]
               â†“                 â†“
        [Coach/App]        [Keep Training]

Tech stack:
- Sensors: Arduino + EMG sensors
- Data: Python + pandas
- Model: scikit-learn SVM (saved .pkl)
- Backend: Flask/FastAPI
- Frontend: Mobile app/Web dashboard
- Alert: Push notifications
```

**Q18: "Cáº§n thÃªm gÃ¬ Ä‘á»ƒ há»‡ thá»‘ng tá»‘t hÆ¡n?"**
```
Tráº£ lá»i:

1. Data:
   âœ“ Thu tháº­p real-world data
   âœ“ TÄƒng sá»‘ samples (10k+)
   âœ“ ThÃªm features: blood lactate, oxygen saturation
   âœ“ Multi-modal sensors

2. Models:
   âœ“ Thá»­ ensemble (Random Forest, XGBoost)
   âœ“ Deep Learning (CNN vá»›i time-series EMG)
   âœ“ Multi-class: Normal/Mild Fatigue/Severe Fatigue

3. Features:
   âœ“ Time-domain: variance, RMS, MAV
   âœ“ Frequency-domain: power spectral density
   âœ“ Temporal: fatigue progression over time

4. Deployment:
   âœ“ Real-time processing (<100ms latency)
   âœ“ Edge computing (on-device model)
   âœ“ Cloud backup vÃ  analytics
   âœ“ User interface design
```

---

#### C. CHECKLIST CHUáº¨N Bá»Š BÃO CÃO

##### âœ… **TÃ€I LIá»†U**

- [ ] Slides PowerPoint (15-20 slides)
- [ ] Code source (Python scripts)
- [ ] BÃ¡o cÃ¡o chi tiáº¿t (Word/PDF, 10-15 trang)
- [ ] Biá»ƒu Ä‘á»“ (confusion matrices, comparison charts)
- [ ] Demo video hoáº·c live demo
- [ ] References (papers, books)

##### âœ… **DEMO**

- [ ] Chuáº©n bá»‹ environment (laptop, projector)
- [ ] Test cháº¡y code trÆ°á»›c
- [ ] Chuáº©n bá»‹ data samples Ä‘á»ƒ demo
- [ ] Script demo sáºµn (copy-paste commands)
- [ ] Backup: video demo náº¿u code lá»—i

##### âœ… **KIáº¾N THá»¨C**

- [ ] Hiá»ƒu rÃµ 3 thuáº­t toÃ¡n (LDA, KNN, SVM)
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c confusion matrix
- [ ] Biáº¿t cÃ¡ch tÃ­nh accuracy, precision, recall, F1
- [ ] Hiá»ƒu cross-validation
- [ ] Biáº¿t GridSearchCV hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c best parameters
- [ ] Náº¯m rÃµ flow cá»§a code

##### âœ… **Tá»° TIN**

- [ ] Luyá»‡n nÃ³i trÆ°á»›c (10-15 phÃºt)
- [ ] Chuáº©n bá»‹ tráº£ lá»i cÃ¢u há»i
- [ ] NÃ³i cháº­m, rÃµ rÃ ng
- [ ] NhÃ¬n vÃ o giÃ¡o viÃªn/audience
- [ ] Tá»± tin vá»›i káº¿t quáº£ (95.73%!)

---

### ğŸ¯ ÄIá»‚M NHáº¤N QUAN TRá»ŒNG KHI BÃO CÃO

#### **1. NHáº¤N Máº NH Káº¾T QUáº¢**
- âœ“ 95.73% accuracy
- âœ“ VÆ°á»£t target 85-95%
- âœ“ SVM tá»‘t nháº¥t
- âœ“ Stable (CV std tháº¥p)

#### **2. GIáº¢I THÃCH RÃ• RÃ€NG**
- Táº¡i sao chá»n features
- Táº¡i sao chá»n algorithms
- CÃ¡ch GridSearchCV hoáº¡t Ä‘á»™ng
- Ã nghÄ©a cÃ¡c metrics

#### **3. THÃ€NH THáº¬T Vá»€ Háº N CHáº¾**
- Data lÃ  synthetic
- Cáº§n real-world validation
- ChÆ°a deploy production
- CÃ³ thá»ƒ improve thÃªm

#### **4. HÆ¯á»šNG PHÃT TRIá»‚N**
- Thu tháº­p real data
- Thá»­ deep learning
- Deploy real-time system
- Clinical validation

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

### Papers:
1. "EMG-based Muscle Fatigue Detection using Machine Learning"
2. "Support Vector Machines for Muscle Fatigue Classification"
3. "Real-time Fatigue Monitoring using Wearable Sensors"

### Books:
1. "Introduction to Machine Learning" - Alpaydin
2. "Pattern Recognition and Machine Learning" - Bishop
3. "The Elements of Statistical Learning" - Hastie et al.

### Online:
1. scikit-learn documentation
2. Towards Data Science blog
3. Machine Learning Mastery

---

## ğŸ”š Káº¾T LUáº¬N

Báº¡n Ä‘Ã£ cÃ³ Ä‘áº§y Ä‘á»§ kiáº¿n thá»©c Ä‘á»ƒ bÃ¡o cÃ¡o giá»¯a ká»³ thÃ nh cÃ´ng!

**Äiá»ƒm máº¡nh cá»§a bÃ i:**
- âœ… Káº¿t quáº£ tá»‘t (95.73%)
- âœ… Code clean, cÃ³ structure
- âœ… Documentation Ä‘áº§y Ä‘á»§
- âœ… Demo dá»… dÃ ng
- âœ… So sÃ¡nh 3 methods

**Tá»± tin lÃªn! ChÃºc báº¡n bÃ¡o cÃ¡o thÃ nh cÃ´ng! ğŸ‰**
